{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8254978,"sourceType":"datasetVersion","datasetId":4739072}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json","metadata":{"id":"qVNa22j50UAc","execution":{"iopub.status.busy":"2024-04-28T18:15:56.630790Z","iopub.execute_input":"2024-04-28T18:15:56.631147Z","iopub.status.idle":"2024-04-28T18:15:56.979875Z","shell.execute_reply.started":"2024-04-28T18:15:56.631117Z","shell.execute_reply":"2024-04-28T18:15:56.979002Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/train_hypothesis_evidences.csv')\ntrain_df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"bSxG_jyC0UAd","outputId":"a2071c31-95cc-4353-95b0-b6b4a1c91676","execution":{"iopub.status.busy":"2024-04-28T18:15:56.981466Z","iopub.execute_input":"2024-04-28T18:15:56.981845Z","iopub.status.idle":"2024-04-28T18:15:57.029802Z","shell.execute_reply.started":"2024-04-28T18:15:56.981819Z","shell.execute_reply":"2024-04-28T18:15:57.028910Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                             hypothesis  \\\n0     All the primary trial participants do not rece...   \n1     Patients with Platelet count over 100,000/mm¬¨...   \n2     Heart-related adverse events were recorded in ...   \n3     Adult Patients with histologic confirmation of...   \n4     Laser Therapy is in each cohort of the primary...   \n...                                                 ...   \n1695  Adequate blood, kidney, and hepatic function a...   \n1696  The Ridaforolimus + Dalotuzumab + Exemestane g...   \n1697  The only difference between the interventions ...   \n1698  Patients must have a white blood cell count ab...   \n1699  the primary trial and the secondary trial both...   \n\n                                              evidences          label  \n0     [\"INTERVENTION 1: \", \"  Diagnostic (FLT PET)\",...  Contradiction  \n1     [\"  PATIENT CHARACTERISTICS:\", \"  ANC  1,500/m...  Contradiction  \n2     [\"Adverse Events 1:\", \"  Supraventricular tach...     Entailment  \n3     [\"Inclusion Criteria:\", \"  Patients with histo...  Contradiction  \n4     [\"INTERVENTION 1: \", \"  Laser Therapy Alone\", ...  Contradiction  \n...                                                 ...            ...  \n1695  [\"Inclusion Criteria:\", \"  Postmenopausal wome...     Entailment  \n1696  [\"Outcome Measurement: \", \"  1. Progression-fr...  Contradiction  \n1697  [\"INTERVENTION 1: \", \"  Prone\", \"Prone positio...     Entailment  \n1698                         [\"  WBC > 1,500/mm\\u00b3\"]     Entailment  \n1699  [\"Outcome Measurement: \", \"  Central Nervous S...  Contradiction  \n\n[1700 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hypothesis</th>\n      <th>evidences</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>All the primary trial participants do not rece...</td>\n      <td>[\"INTERVENTION 1: \", \"  Diagnostic (FLT PET)\",...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Patients with Platelet count over 100,000/mm¬¨...</td>\n      <td>[\"  PATIENT CHARACTERISTICS:\", \"  ANC  1,500/m...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Heart-related adverse events were recorded in ...</td>\n      <td>[\"Adverse Events 1:\", \"  Supraventricular tach...</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adult Patients with histologic confirmation of...</td>\n      <td>[\"Inclusion Criteria:\", \"  Patients with histo...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Laser Therapy is in each cohort of the primary...</td>\n      <td>[\"INTERVENTION 1: \", \"  Laser Therapy Alone\", ...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1695</th>\n      <td>Adequate blood, kidney, and hepatic function a...</td>\n      <td>[\"Inclusion Criteria:\", \"  Postmenopausal wome...</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>1696</th>\n      <td>The Ridaforolimus + Dalotuzumab + Exemestane g...</td>\n      <td>[\"Outcome Measurement: \", \"  1. Progression-fr...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>1697</th>\n      <td>The only difference between the interventions ...</td>\n      <td>[\"INTERVENTION 1: \", \"  Prone\", \"Prone positio...</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>1698</th>\n      <td>Patients must have a white blood cell count ab...</td>\n      <td>[\"  WBC &gt; 1,500/mm\\u00b3\"]</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>1699</th>\n      <td>the primary trial and the secondary trial both...</td>\n      <td>[\"Outcome Measurement: \", \"  Central Nervous S...</td>\n      <td>Contradiction</td>\n    </tr>\n  </tbody>\n</table>\n<p>1700 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"hypothesis_lst=train_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=train_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=train_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKUARfOA0UAe","outputId":"d366615e-bf5c-4346-ea83-05066884183b","execution":{"iopub.status.busy":"2024-04-28T18:15:57.030872Z","iopub.execute_input":"2024-04-28T18:15:57.031194Z","iopub.status.idle":"2024-04-28T18:15:57.053367Z","shell.execute_reply.started":"2024-04-28T18:15:57.031168Z","shell.execute_reply":"2024-04-28T18:15:57.052489Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"1700"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport math\nfrom torch.utils.data import Dataset\n\nclass InputSequence(Dataset):\n    def __init__(self, tok, l_text, l_text2, l_label=None, batch_size=64, gpu=True):\n        self.data_len = len(l_text)\n        self.data_idx = list(range(self.data_len))\n        self.texts = tok(l_text, l_text2, padding=True, truncation=True, max_length=512, return_tensors='pt')\n        self.l_label = np.array(l_label) if l_label is not None else None  # Initialize labels only if provided\n        self.batch_size = batch_size\n        self.gpu = gpu\n        print('Tokenization done')\n\n    def on_epoch_end(self):\n        random.shuffle(self.data_idx)\n\n    def __getitem__(self, i):\n        start = i * self.batch_size\n        batch_idx = self.data_idx[start:min(start + self.batch_size, self.data_len)]\n\n        # Convert tensors to CUDA if GPU is enabled\n        return_texts = {k: self.texts[k][batch_idx].cuda() if self.gpu else self.texts[k][batch_idx] for k in self.texts}\n\n        if self.l_label is not None:  # Check if labels were provided\n            return_labels = torch.from_numpy(self.l_label[batch_idx].astype(np.int64))\n            if self.gpu:\n                return_labels = return_labels.cuda()\n            return return_texts, return_labels\n        else:\n            return return_texts  # Only return texts if no labels are available\n\n    def __len__(self):\n        return math.ceil(self.data_len / self.batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:15:57.055394Z","iopub.execute_input":"2024-04-28T18:15:57.055694Z","iopub.status.idle":"2024-04-28T18:15:58.816097Z","shell.execute_reply.started":"2024-04-28T18:15:57.055671Z","shell.execute_reply":"2024-04-28T18:15:58.815173Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\nimport math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:15:58.817248Z","iopub.execute_input":"2024-04-28T18:15:58.817709Z","iopub.status.idle":"2024-04-28T18:15:59.827250Z","shell.execute_reply.started":"2024-04-28T18:15:58.817678Z","shell.execute_reply":"2024-04-28T18:15:59.826261Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntext_tok = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\ntext_clf = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\ntext_clf.num_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:15:59.828496Z","iopub.execute_input":"2024-04-28T18:15:59.829149Z","iopub.status.idle":"2024-04-28T18:16:01.878687Z","shell.execute_reply.started":"2024-04-28T18:15:59.829113Z","shell.execute_reply":"2024-04-28T18:16:01.877551Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"108311810"},"metadata":{}}]},{"cell_type":"code","source":"training_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)\nlen(training_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:16:01.880234Z","iopub.execute_input":"2024-04-28T18:16:01.881109Z","iopub.status.idle":"2024-04-28T18:16:03.287706Z","shell.execute_reply.started":"2024-04-28T18:16:01.881069Z","shell.execute_reply":"2024-04-28T18:16:03.286761Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Tokenization done\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"27"},"metadata":{}}]},{"cell_type":"code","source":"dev_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/dev_hypothesis_evidences.csv')\ndev_df","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:16:03.288759Z","iopub.execute_input":"2024-04-28T18:16:03.289014Z","iopub.status.idle":"2024-04-28T18:16:03.306011Z","shell.execute_reply.started":"2024-04-28T18:16:03.288992Z","shell.execute_reply":"2024-04-28T18:16:03.305109Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                            hypothesis  \\\n0    there is a 13.2% difference between the result...   \n1    Patients with significantly elevated ejection ...   \n2    a significant number of the participants in th...   \n3    the primary trial does not report the PFS or o...   \n4    Prior treatment with fulvestrant or with a pho...   \n..                                                 ...   \n195  The the primary trial intervention involves on...   \n196  the secondary trial reported 1 single case of ...   \n197  the secondary trial and the primary trial do n...   \n198  the outcome measurement of the primary trial i...   \n199  All the primary trial patients had a minimum o...   \n\n                                             evidences          label  \n0    [\"Outcome Measurement: \", \"  Event-free Surviv...  Contradiction  \n1    [\"  Cardiac left ventricular function with res...  Contradiction  \n2    [\"  Enterocolitis 1/167 (0.60%)\", \"  Enterocol...  Contradiction  \n3    [\"Outcome Measurement: \", \"  Local Control Usi...     Entailment  \n4    [\"  Prior treatment with a phosphatidylinosito...  Contradiction  \n..                                                 ...            ...  \n195  [\"INTERVENTION 1: \", \"  Letrozole\", \"  Partici...  Contradiction  \n196  [\"Adverse Events 1:\", \"  Total: 16/48 (33.33%)...     Entailment  \n197  [\"Outcome Measurement: \", \"  Number of Patient...     Entailment  \n198  [\"Outcome Measurement: \", \"  Progression-free ...     Entailment  \n199  [\"Outcome Measurement: \", \"  Complete Response...  Contradiction  \n\n[200 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hypothesis</th>\n      <th>evidences</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>there is a 13.2% difference between the result...</td>\n      <td>[\"Outcome Measurement: \", \"  Event-free Surviv...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Patients with significantly elevated ejection ...</td>\n      <td>[\"  Cardiac left ventricular function with res...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a significant number of the participants in th...</td>\n      <td>[\"  Enterocolitis 1/167 (0.60%)\", \"  Enterocol...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the primary trial does not report the PFS or o...</td>\n      <td>[\"Outcome Measurement: \", \"  Local Control Usi...</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Prior treatment with fulvestrant or with a pho...</td>\n      <td>[\"  Prior treatment with a phosphatidylinosito...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>The the primary trial intervention involves on...</td>\n      <td>[\"INTERVENTION 1: \", \"  Letrozole\", \"  Partici...</td>\n      <td>Contradiction</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>the secondary trial reported 1 single case of ...</td>\n      <td>[\"Adverse Events 1:\", \"  Total: 16/48 (33.33%)...</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>the secondary trial and the primary trial do n...</td>\n      <td>[\"Outcome Measurement: \", \"  Number of Patient...</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>the outcome measurement of the primary trial i...</td>\n      <td>[\"Outcome Measurement: \", \"  Progression-free ...</td>\n      <td>Entailment</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>All the primary trial patients had a minimum o...</td>\n      <td>[\"Outcome Measurement: \", \"  Complete Response...</td>\n      <td>Contradiction</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"hypothesis_lst=dev_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=dev_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=dev_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:16:03.307108Z","iopub.execute_input":"2024-04-28T18:16:03.307413Z","iopub.status.idle":"2024-04-28T18:16:03.317599Z","shell.execute_reply.started":"2024-04-28T18:16:03.307390Z","shell.execute_reply":"2024-04-28T18:16:03.316685Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"200"},"metadata":{}}]},{"cell_type":"code","source":"dev_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:16:03.321098Z","iopub.execute_input":"2024-04-28T18:16:03.321358Z","iopub.status.idle":"2024-04-28T18:16:03.480330Z","shell.execute_reply.started":"2024-04-28T18:16:03.321335Z","shell.execute_reply":"2024-04-28T18:16:03.479331Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Tokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,clf):\n        super(Model, self).__init__()\n        self.clf=clf\n        self.loss=nn.CrossEntropyLoss()\n\n    def forward(self, texts, labels, gpu=True):\n\n        loss=self.loss(self.clf(**texts).logits, labels)\n\n        return loss","metadata":{"id":"Z3jMfFZO0UAi","execution":{"iopub.status.busy":"2024-04-28T18:16:03.481669Z","iopub.execute_input":"2024-04-28T18:16:03.482061Z","iopub.status.idle":"2024-04-28T18:16:03.488104Z","shell.execute_reply.started":"2024-04-28T18:16:03.482025Z","shell.execute_reply":"2024-04-28T18:16:03.487114Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model=Model(text_clf)","metadata":{"id":"GH6lpn1b0UAi","execution":{"iopub.status.busy":"2024-04-28T18:16:03.489288Z","iopub.execute_input":"2024-04-28T18:16:03.489541Z","iopub.status.idle":"2024-04-28T18:16:03.498797Z","shell.execute_reply.started":"2024-04-28T18:16:03.489519Z","shell.execute_reply":"2024-04-28T18:16:03.497964Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport torch\nfrom torch import nn\nfrom transformers import AdamW\nimport numpy as np\nimport math\nimport random\n\nbat_s = 16\nl_rate = 1e-5\n\ntraining_data.batch_size = bat_s\n\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=l_rate)\ntotal_epoch_num = 10\n\nfor epoch in range(total_epoch_num):\n    training_data.on_epoch_end()\n    loss_sum = 0.0\n    loss_count = 0\n    model.train()  # Ensure model is in training mode\n    \n    for batch in range(len(training_data)):\n        optimizer.zero_grad()\n        batch_texts, batch_labels = training_data[batch]\n        loss_count += len(batch_texts['input_ids'])\n        loss = model(batch_texts, batch_labels)\n        print('epoch:', epoch, 'batch:', batch, 'loss:', loss.item(), end='\\n' if batch == 0 or batch + 1 == len(training_data) or (batch + 1) % 1000 == 0 else '\\r')\n        loss_sum += loss.item() * len(batch_texts['input_ids'])\n        loss.backward()\n        optimizer.step()\n        \n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n    val_loss_sum = 0.0\n    val_loss_count = 0\n    with torch.no_grad():\n        for batch in range(len(dev_data)):\n            batch_texts, batch_labels = dev_data[batch]\n            val_loss_count += len(batch_texts['input_ids'])\n            val_loss = model(batch_texts, batch_labels)\n            val_loss_sum += val_loss.item() * len(batch_texts['input_ids'])\n\n    avg_val_loss = val_loss_sum / val_loss_count\n    print(f'Validation - Epoch: {epoch}, Avg Loss: {avg_val_loss}')\n\n    # Save the model after each epoch\n    model_path = f'./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{epoch:05d}.pt'\n    model.clf.save_pretrained(model_path)\n    print(f'Model saved to {model_path}')\n\n_ = model.cpu()  # Move model back to CPU if necessary","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVPWV20I0UAj","outputId":"05baa987-48ef-4ee5-f663-a52fc2beada7","execution":{"iopub.status.busy":"2024-04-28T18:16:03.499996Z","iopub.execute_input":"2024-04-28T18:16:03.500545Z","iopub.status.idle":"2024-04-28T18:31:26.393211Z","shell.execute_reply.started":"2024-04-28T18:16:03.500513Z","shell.execute_reply":"2024-04-28T18:31:26.392370Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"epoch: 0 batch: 0 loss: 0.64802086353302\nepoch: 0 batch: 106 loss: 0.7162390947341919\nValidation - Epoch: 0, Avg Loss: 0.694146432876587\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00000.pt\nepoch: 1 batch: 0 loss: 0.6931287050247192\nepoch: 1 batch: 106 loss: 0.7450661063194275\nValidation - Epoch: 1, Avg Loss: 0.6926659679412842\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00001.pt\nepoch: 2 batch: 0 loss: 0.6728920936584473\nepoch: 2 batch: 106 loss: 0.8213199973106384\nValidation - Epoch: 2, Avg Loss: 0.69583087682724\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00002.pt\nepoch: 3 batch: 0 loss: 0.7436635494232178\nepoch: 3 batch: 106 loss: 0.6952358484268188\nValidation - Epoch: 3, Avg Loss: 0.6885556483268738\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00003.pt\nepoch: 4 batch: 0 loss: 0.6733831763267517\nepoch: 4 batch: 106 loss: 0.7267605066299438\nValidation - Epoch: 4, Avg Loss: 0.676571147441864\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00004.pt\nepoch: 5 batch: 0 loss: 0.7108661532402039\nepoch: 5 batch: 106 loss: 1.1244339942932137\nValidation - Epoch: 5, Avg Loss: 0.6876934337615966\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00005.pt\nepoch: 6 batch: 0 loss: 0.695556640625\nepoch: 6 batch: 106 loss: 0.8106969594955444\nValidation - Epoch: 6, Avg Loss: 0.6649718451499939\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00006.pt\nepoch: 7 batch: 0 loss: 0.6660974621772766\nepoch: 7 batch: 106 loss: 0.6477980613708496\nValidation - Epoch: 7, Avg Loss: 0.6583775877952576\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00007.pt\nepoch: 8 batch: 0 loss: 0.6240636110305786\nepoch: 8 batch: 106 loss: 1.0437890291213995\nValidation - Epoch: 8, Avg Loss: 0.6580778336524964\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00008.pt\nepoch: 9 batch: 0 loss: 0.6131030321121216\nepoch: 9 batch: 106 loss: 1.0355458259582527\nValidation - Epoch: 9, Avg Loss: 0.655454409122467\nModel saved to ./clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_00009.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(dev_data)):\n            batch_texts, batch_labels = dev_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:31:26.394410Z","iopub.execute_input":"2024-04-28T18:31:26.394723Z","iopub.status.idle":"2024-04-28T18:32:10.150183Z","shell.execute_reply.started":"2024-04-28T18:31:26.394696Z","shell.execute_reply":"2024-04-28T18:32:10.149148Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:10.151674Z","iopub.execute_input":"2024-04-28T18:32:10.152404Z","iopub.status.idle":"2024-04-28T18:32:10.250095Z","shell.execute_reply.started":"2024-04-28T18:32:10.152364Z","shell.execute_reply":"2024-04-28T18:32:10.249211Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC   REC    ACC  Faithfulness  \\\n0   pretrained  0.508716  0.516432  0.486726  0.55  0.485      0.414062   \n1            1  0.533684  0.666667  0.500000  1.00  0.500      0.492188   \n2            2  0.533392  0.535211  0.504425  0.57  0.505      0.546875   \n3            3  0.541965  0.666667  0.500000  1.00  0.500      0.492188   \n4            4  0.553566  0.633205  0.515723  0.82  0.525      0.484375   \n5            5  0.635853  0.645669  0.532468  0.82  0.550      0.500000   \n6            6  0.650941  0.659341  0.520231  0.90  0.535      0.468750   \n7            7  0.653969  0.616740  0.551181  0.70  0.565      0.476562   \n8            8  0.690757  0.650407  0.547945  0.80  0.570      0.492188   \n9            9  0.667787  0.548780  0.703125  0.45  0.630      0.460938   \n10          10  0.651321  0.527607  0.682540  0.43  0.615      0.406250   \n\n    Consistency  \n0      0.585938  \n1      0.507812  \n2      0.453125  \n3      0.507812  \n4      0.515625  \n5      0.500000  \n6      0.531250  \n7      0.523438  \n8      0.507812  \n9      0.539062  \n10     0.593750  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.508716</td>\n      <td>0.516432</td>\n      <td>0.486726</td>\n      <td>0.55</td>\n      <td>0.485</td>\n      <td>0.414062</td>\n      <td>0.585938</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.533684</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.00</td>\n      <td>0.500</td>\n      <td>0.492188</td>\n      <td>0.507812</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.533392</td>\n      <td>0.535211</td>\n      <td>0.504425</td>\n      <td>0.57</td>\n      <td>0.505</td>\n      <td>0.546875</td>\n      <td>0.453125</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.541965</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.00</td>\n      <td>0.500</td>\n      <td>0.492188</td>\n      <td>0.507812</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.553566</td>\n      <td>0.633205</td>\n      <td>0.515723</td>\n      <td>0.82</td>\n      <td>0.525</td>\n      <td>0.484375</td>\n      <td>0.515625</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.635853</td>\n      <td>0.645669</td>\n      <td>0.532468</td>\n      <td>0.82</td>\n      <td>0.550</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.650941</td>\n      <td>0.659341</td>\n      <td>0.520231</td>\n      <td>0.90</td>\n      <td>0.535</td>\n      <td>0.468750</td>\n      <td>0.531250</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.653969</td>\n      <td>0.616740</td>\n      <td>0.551181</td>\n      <td>0.70</td>\n      <td>0.565</td>\n      <td>0.476562</td>\n      <td>0.523438</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.690757</td>\n      <td>0.650407</td>\n      <td>0.547945</td>\n      <td>0.80</td>\n      <td>0.570</td>\n      <td>0.492188</td>\n      <td>0.507812</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.667787</td>\n      <td>0.548780</td>\n      <td>0.703125</td>\n      <td>0.45</td>\n      <td>0.630</td>\n      <td>0.460938</td>\n      <td>0.539062</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.651321</td>\n      <td>0.527607</td>\n      <td>0.682540</td>\n      <td>0.43</td>\n      <td>0.615</td>\n      <td>0.406250</td>\n      <td>0.593750</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for dev data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/Numerical_Statements_hypothesis_evidences.csv')\nprint(numerical_df)\n\nhypothesis_lst=numerical_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=numerical_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=numerical_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\nnumerical_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:10.251238Z","iopub.execute_input":"2024-04-28T18:32:10.251526Z","iopub.status.idle":"2024-04-28T18:32:10.353368Z","shell.execute_reply.started":"2024-04-28T18:32:10.251500Z","shell.execute_reply":"2024-04-28T18:32:10.352512Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"                                           hypothesis  \\\n0   there is a 13.2% difference between the result...   \n1   Patients with significantly elevated ejection ...   \n2   Prior treatment with fulvestrant or with a pho...   \n3   only patients with a HER2-positive status can ...   \n4   the shortest PFS in cohort 1 of the primary tr...   \n..                                                ...   \n88  All the primary trial patients achieved either...   \n89  A patient with a node positive T2 N2 M0 adenoc...   \n90  the primary trial results show that the Trastu...   \n91  the secondary trial reported 1 single case of ...   \n92  All the primary trial patients had a minimum o...   \n\n                                            evidences          label  \n0   [\"Outcome Measurement: \", \"  Event-free Surviv...  Contradiction  \n1   [\"  Cardiac left ventricular function with res...  Contradiction  \n2   [\"  Prior treatment with a phosphatidylinosito...  Contradiction  \n3   [\"  HER2-positive status (patients who have un...  Contradiction  \n4   [\"Outcome Measurement: \", \"  Progression-Free ...  Contradiction  \n..                                                ...            ...  \n88  [\"Outcome Measurement: \", \"  Complete Response...     Entailment  \n89  [\"  node-positive: T1-3, N1-2, M0 (level of T ...  Contradiction  \n90  [\"Outcome Measurement: \", \"  Number of Partici...  Contradiction  \n91  [\"Adverse Events 1:\", \"  Total: 16/48 (33.33%)...     Entailment  \n92  [\"Outcome Measurement: \", \"  Complete Response...  Contradiction  \n\n[93 rows x 3 columns]\nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(numerical_data)):\n            batch_texts, batch_labels = numerical_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:10.354596Z","iopub.execute_input":"2024-04-28T18:32:10.354965Z","iopub.status.idle":"2024-04-28T18:32:32.770485Z","shell.execute_reply.started":"2024-04-28T18:32:10.354937Z","shell.execute_reply":"2024-04-28T18:32:32.769455Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:32.771657Z","iopub.execute_input":"2024-04-28T18:32:32.771934Z","iopub.status.idle":"2024-04-28T18:32:32.860248Z","shell.execute_reply.started":"2024-04-28T18:32:32.771910Z","shell.execute_reply":"2024-04-28T18:32:32.859207Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.466792  0.617886  0.469136  0.904762  0.494624   \n1            1  0.508319  0.622222  0.451613  1.000000  0.451613   \n2            2  0.440028  0.468085  0.423077  0.523810  0.462366   \n3            3  0.486060  0.622222  0.451613  1.000000  0.451613   \n4            4  0.487970  0.584071  0.464789  0.785714  0.494624   \n5            5  0.538937  0.630631  0.507246  0.833333  0.559140   \n6            6  0.523688  0.600000  0.461538  0.857143  0.483871   \n7            7  0.526483  0.527473  0.489796  0.571429  0.537634   \n8            8  0.611952  0.581818  0.470588  0.761905  0.505376   \n9            9  0.596912  0.393443  0.631579  0.285714  0.602151   \n10          10  0.576622  0.419355  0.650000  0.309524  0.612903   \n\n    Faithfulness  Consistency  \n0       0.578125     0.421875  \n1       0.515625     0.484375  \n2       0.546875     0.453125  \n3       0.515625     0.484375  \n4       0.515625     0.484375  \n5       0.468750     0.531250  \n6       0.515625     0.484375  \n7       0.406250     0.593750  \n8       0.406250     0.593750  \n9       0.500000     0.500000  \n10      0.437500     0.562500  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.466792</td>\n      <td>0.617886</td>\n      <td>0.469136</td>\n      <td>0.904762</td>\n      <td>0.494624</td>\n      <td>0.578125</td>\n      <td>0.421875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.508319</td>\n      <td>0.622222</td>\n      <td>0.451613</td>\n      <td>1.000000</td>\n      <td>0.451613</td>\n      <td>0.515625</td>\n      <td>0.484375</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.440028</td>\n      <td>0.468085</td>\n      <td>0.423077</td>\n      <td>0.523810</td>\n      <td>0.462366</td>\n      <td>0.546875</td>\n      <td>0.453125</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.486060</td>\n      <td>0.622222</td>\n      <td>0.451613</td>\n      <td>1.000000</td>\n      <td>0.451613</td>\n      <td>0.515625</td>\n      <td>0.484375</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.487970</td>\n      <td>0.584071</td>\n      <td>0.464789</td>\n      <td>0.785714</td>\n      <td>0.494624</td>\n      <td>0.515625</td>\n      <td>0.484375</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.538937</td>\n      <td>0.630631</td>\n      <td>0.507246</td>\n      <td>0.833333</td>\n      <td>0.559140</td>\n      <td>0.468750</td>\n      <td>0.531250</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.523688</td>\n      <td>0.600000</td>\n      <td>0.461538</td>\n      <td>0.857143</td>\n      <td>0.483871</td>\n      <td>0.515625</td>\n      <td>0.484375</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.526483</td>\n      <td>0.527473</td>\n      <td>0.489796</td>\n      <td>0.571429</td>\n      <td>0.537634</td>\n      <td>0.406250</td>\n      <td>0.593750</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.611952</td>\n      <td>0.581818</td>\n      <td>0.470588</td>\n      <td>0.761905</td>\n      <td>0.505376</td>\n      <td>0.406250</td>\n      <td>0.593750</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.596912</td>\n      <td>0.393443</td>\n      <td>0.631579</td>\n      <td>0.285714</td>\n      <td>0.602151</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.576622</td>\n      <td>0.419355</td>\n      <td>0.650000</td>\n      <td>0.309524</td>\n      <td>0.612903</td>\n      <td>0.437500</td>\n      <td>0.562500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for numerical data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_numerical_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/Non_Numerical_Statements_hypothesis_evidences.csv')\nprint(non_numerical_df)\n\nhypothesis_lst=non_numerical_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=non_numerical_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=non_numerical_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\nnon_numerical_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:32.861609Z","iopub.execute_input":"2024-04-28T18:32:32.862265Z","iopub.status.idle":"2024-04-28T18:32:32.968523Z","shell.execute_reply.started":"2024-04-28T18:32:32.862226Z","shell.execute_reply":"2024-04-28T18:32:32.967617Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"                                            hypothesis  \\\n0    a significant number of the participants in th...   \n1    the primary trial does not report the PFS or o...   \n2    the outcome measurement of the primary trial i...   \n3    The the primary trial results section reports ...   \n4    both cohorts of the primary trial receive iden...   \n..                                                 ...   \n102  Patients must be bedbound or severaly disabled...   \n103  the results of the secondary trial and the pri...   \n104  The the primary trial intervention involves on...   \n105  the secondary trial and the primary trial do n...   \n106  the outcome measurement of the primary trial i...   \n\n                                             evidences          label  \n0    [\"  Enterocolitis 1/167 (0.60%)\", \"  Enterocol...  Contradiction  \n1    [\"Outcome Measurement: \", \"  Local Control Usi...     Entailment  \n2    [\"Outcome Measurement: \", \"  Progression-free ...  Contradiction  \n3    [\"Outcome Measurement: \", \"  Proportion of Sen...     Entailment  \n4    [\"INTERVENTION 1: \", \"  Nivolumab + Daratumuma...     Entailment  \n..                                                 ...            ...  \n102  [\"Inclusion Criteria:\", \"  Pathologically conf...  Contradiction  \n103  [\"Outcome Measurement: \", \"  Safety of Externa...  Contradiction  \n104  [\"INTERVENTION 1: \", \"  Letrozole\", \"  Partici...  Contradiction  \n105  [\"Outcome Measurement: \", \"  Number of Patient...     Entailment  \n106  [\"Outcome Measurement: \", \"  Progression-free ...     Entailment  \n\n[107 rows x 3 columns]\nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(non_numerical_data)):\n            batch_texts, batch_labels = non_numerical_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:32.969775Z","iopub.execute_input":"2024-04-28T18:32:32.970135Z","iopub.status.idle":"2024-04-28T18:32:57.850187Z","shell.execute_reply.started":"2024-04-28T18:32:32.970099Z","shell.execute_reply":"2024-04-28T18:32:57.849132Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:57.851350Z","iopub.execute_input":"2024-04-28T18:32:57.851669Z","iopub.status.idle":"2024-04-28T18:32:57.942603Z","shell.execute_reply.started":"2024-04-28T18:32:57.851623Z","shell.execute_reply":"2024-04-28T18:32:57.941673Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.562641  0.000000  0.000000  0.000000  0.457944   \n1            1  0.568111  0.703030  0.542056  1.000000  0.542056   \n2            2  0.616581  0.588235  0.573770  0.603448  0.542056   \n3            3  0.588396  0.703030  0.542056  1.000000  0.542056   \n4            4  0.615219  0.671233  0.556818  0.844828  0.551402   \n5            5  0.698396  0.657343  0.552941  0.810345  0.542056   \n6            6  0.726224  0.705882  0.568421  0.931034  0.579439   \n7            7  0.722123  0.676471  0.589744  0.793103  0.588785   \n8            8  0.745150  0.705882  0.615385  0.827586  0.626168   \n9            9  0.718417  0.640777  0.733333  0.568966  0.654206   \n10          10  0.709786  0.594059  0.697674  0.517241  0.616822   \n\n    Faithfulness  Consistency  \n0       0.520000     0.480000  \n1       0.480000     0.520000  \n2       0.440000     0.560000  \n3       0.480000     0.520000  \n4       0.426667     0.573333  \n5       0.440000     0.560000  \n6       0.386667     0.613333  \n7       0.426667     0.573333  \n8       0.360000     0.640000  \n9       0.386667     0.613333  \n10      0.360000     0.640000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.562641</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.457944</td>\n      <td>0.520000</td>\n      <td>0.480000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.568111</td>\n      <td>0.703030</td>\n      <td>0.542056</td>\n      <td>1.000000</td>\n      <td>0.542056</td>\n      <td>0.480000</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.616581</td>\n      <td>0.588235</td>\n      <td>0.573770</td>\n      <td>0.603448</td>\n      <td>0.542056</td>\n      <td>0.440000</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.588396</td>\n      <td>0.703030</td>\n      <td>0.542056</td>\n      <td>1.000000</td>\n      <td>0.542056</td>\n      <td>0.480000</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.615219</td>\n      <td>0.671233</td>\n      <td>0.556818</td>\n      <td>0.844828</td>\n      <td>0.551402</td>\n      <td>0.426667</td>\n      <td>0.573333</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.698396</td>\n      <td>0.657343</td>\n      <td>0.552941</td>\n      <td>0.810345</td>\n      <td>0.542056</td>\n      <td>0.440000</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.726224</td>\n      <td>0.705882</td>\n      <td>0.568421</td>\n      <td>0.931034</td>\n      <td>0.579439</td>\n      <td>0.386667</td>\n      <td>0.613333</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.722123</td>\n      <td>0.676471</td>\n      <td>0.589744</td>\n      <td>0.793103</td>\n      <td>0.588785</td>\n      <td>0.426667</td>\n      <td>0.573333</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.745150</td>\n      <td>0.705882</td>\n      <td>0.615385</td>\n      <td>0.827586</td>\n      <td>0.626168</td>\n      <td>0.360000</td>\n      <td>0.640000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.718417</td>\n      <td>0.640777</td>\n      <td>0.733333</td>\n      <td>0.568966</td>\n      <td>0.654206</td>\n      <td>0.386667</td>\n      <td>0.613333</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.709786</td>\n      <td>0.594059</td>\n      <td>0.697674</td>\n      <td>0.517241</td>\n      <td>0.616822</td>\n      <td>0.360000</td>\n      <td>0.640000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for Non numerical data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/Single_hypothesis_evidences.csv')\nprint(single_df)\n\nhypothesis_lst=single_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=single_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=single_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\nsingle_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:57.943839Z","iopub.execute_input":"2024-04-28T18:32:57.944111Z","iopub.status.idle":"2024-04-28T18:32:58.070868Z","shell.execute_reply.started":"2024-04-28T18:32:57.944087Z","shell.execute_reply":"2024-04-28T18:32:58.069926Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"                                            hypothesis  \\\n0    there is a 13.2% difference between the result...   \n1    the primary trial does not report the PFS or o...   \n2    Prior treatment with fulvestrant or with a pho...   \n3    only patients with a HER2-positive status can ...   \n4    the shortest PFS in cohort 1 of the primary tr...   \n..                                                 ...   \n135  A patient with a node positive T2 N2 M0 adenoc...   \n136  Patients must be bedbound or severaly disabled...   \n137  the primary trial results show that the Trastu...   \n138  the outcome measurement of the primary trial i...   \n139  All the primary trial patients had a minimum o...   \n\n                                             evidences          label  \n0    [\"Outcome Measurement: \", \"  Event-free Surviv...  Contradiction  \n1    [\"Outcome Measurement: \", \"  Local Control Usi...     Entailment  \n2    [\"  Prior treatment with a phosphatidylinosito...  Contradiction  \n3    [\"  HER2-positive status (patients who have un...  Contradiction  \n4    [\"Outcome Measurement: \", \"  Progression-Free ...  Contradiction  \n..                                                 ...            ...  \n135  [\"  node-positive: T1-3, N1-2, M0 (level of T ...  Contradiction  \n136  [\"Inclusion Criteria:\", \"  Pathologically conf...  Contradiction  \n137  [\"Outcome Measurement: \", \"  Number of Partici...  Contradiction  \n138  [\"Outcome Measurement: \", \"  Progression-free ...     Entailment  \n139  [\"Outcome Measurement: \", \"  Complete Response...  Contradiction  \n\n[140 rows x 3 columns]\nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(single_data)):\n            batch_texts, batch_labels = single_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:32:58.072270Z","iopub.execute_input":"2024-04-28T18:32:58.072574Z","iopub.status.idle":"2024-04-28T18:33:29.339621Z","shell.execute_reply.started":"2024-04-28T18:32:58.072551Z","shell.execute_reply":"2024-04-28T18:33:29.338605Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:29.340948Z","iopub.execute_input":"2024-04-28T18:33:29.341906Z","iopub.status.idle":"2024-04-28T18:33:29.433515Z","shell.execute_reply.started":"2024-04-28T18:33:29.341875Z","shell.execute_reply":"2024-04-28T18:33:29.432670Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.515581  0.609195  0.509615  0.757143  0.514286   \n1            1  0.542686  0.666667  0.500000  1.000000  0.500000   \n2            2  0.557420  0.530612  0.506494  0.557143  0.507143   \n3            3  0.556162  0.666667  0.500000  1.000000  0.500000   \n4            4  0.560455  0.645503  0.512605  0.871429  0.521429   \n5            5  0.604035  0.637838  0.513043  0.842857  0.521429   \n6            6  0.606682  0.649485  0.508065  0.900000  0.514286   \n7            7  0.628146  0.634731  0.546392  0.757143  0.564286   \n8            8  0.658606  0.659459  0.530435  0.871429  0.550000   \n9            9  0.643733  0.542373  0.666667  0.457143  0.614286   \n10          10  0.629105  0.534483  0.673913  0.442857  0.614286   \n\n    Faithfulness  Consistency  \n0       0.427083     0.572917  \n1       0.458333     0.541667  \n2       0.385417     0.614583  \n3       0.458333     0.541667  \n4       0.500000     0.500000  \n5       0.541667     0.458333  \n6       0.500000     0.500000  \n7       0.510417     0.489583  \n8       0.500000     0.500000  \n9       0.468750     0.531250  \n10      0.416667     0.583333  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.515581</td>\n      <td>0.609195</td>\n      <td>0.509615</td>\n      <td>0.757143</td>\n      <td>0.514286</td>\n      <td>0.427083</td>\n      <td>0.572917</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.542686</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.458333</td>\n      <td>0.541667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.557420</td>\n      <td>0.530612</td>\n      <td>0.506494</td>\n      <td>0.557143</td>\n      <td>0.507143</td>\n      <td>0.385417</td>\n      <td>0.614583</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.556162</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.458333</td>\n      <td>0.541667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.560455</td>\n      <td>0.645503</td>\n      <td>0.512605</td>\n      <td>0.871429</td>\n      <td>0.521429</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.604035</td>\n      <td>0.637838</td>\n      <td>0.513043</td>\n      <td>0.842857</td>\n      <td>0.521429</td>\n      <td>0.541667</td>\n      <td>0.458333</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.606682</td>\n      <td>0.649485</td>\n      <td>0.508065</td>\n      <td>0.900000</td>\n      <td>0.514286</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.628146</td>\n      <td>0.634731</td>\n      <td>0.546392</td>\n      <td>0.757143</td>\n      <td>0.564286</td>\n      <td>0.510417</td>\n      <td>0.489583</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.658606</td>\n      <td>0.659459</td>\n      <td>0.530435</td>\n      <td>0.871429</td>\n      <td>0.550000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.643733</td>\n      <td>0.542373</td>\n      <td>0.666667</td>\n      <td>0.457143</td>\n      <td>0.614286</td>\n      <td>0.468750</td>\n      <td>0.531250</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.629105</td>\n      <td>0.534483</td>\n      <td>0.673913</td>\n      <td>0.442857</td>\n      <td>0.614286</td>\n      <td>0.416667</td>\n      <td>0.583333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for Single Type data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/Comparison_hypothesis_evidences.csv')\nprint(comparison_df)\n\nhypothesis_lst=comparison_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=comparison_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=comparison_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\ncomparison_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:29.434910Z","iopub.execute_input":"2024-04-28T18:33:29.435267Z","iopub.status.idle":"2024-04-28T18:33:29.508997Z","shell.execute_reply.started":"2024-04-28T18:33:29.435234Z","shell.execute_reply":"2024-04-28T18:33:29.508126Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"                                           hypothesis  \\\n0   Patients with significantly elevated ejection ...   \n1   a significant number of the participants in th...   \n2   Diarrhoea is the most common adverse event in ...   \n3   most participants in the secondary trial and t...   \n4   Presence of hot flashes for at least 4 month p...   \n5   the secondary trial and the primary trial both...   \n6   the primary trial and the secondary trial use ...   \n7   the primary trial and the secondary trial do n...   \n8   the secondary trial and the primary trial do n...   \n9   Between the secondary trial and the primary tr...   \n10  All patients in the primary trial receive high...   \n11  all participants of the primary trial have sta...   \n12  The results from the secondary trial and the p...   \n13  the primary trial and the secondary trial part...   \n14  There are more cases of Urosepsis in the prima...   \n15  the primary trial participants are not adminis...   \n16  All the primary trial participants are adminis...   \n17  the primary trial and the secondary trial do n...   \n18  All the primary trial participants are adminis...   \n19  the primary trial participants are administere...   \n20  Patients must be female and have confirmed adv...   \n21  the primary trial and the secondary trial repo...   \n22  the primary trial measures the Percentage of P...   \n23  neither the secondary trial or the primary tri...   \n24  the secondary trial and the primary trial use ...   \n25  the secondary trial and the primary trial do n...   \n26  the primary trial and the secondary trial do n...   \n27  There less cases of Acute myocardial infarctio...   \n28  the secondary trial reported 1 single case of ...   \n29  Patients with significantly low ejection fract...   \n30  all participants of the primary trial have sta...   \n31  the primary trial and the secondary trial use ...   \n32  Women currently undergoing endocrine therapy a...   \n33  the primary trial and the secondary trial part...   \n34  the primary trial and the secondary trial repo...   \n35  the results of the secondary trial and the pri...   \n36  There were more cases of mental health disorde...   \n37  The results from the secondary trial and the p...   \n38  There is one case of Diarrhea in the secondary...   \n39  Presence of hot flashes for  30 days prior to ...   \n40  the primary trial and the secondary trial meas...   \n41  The the primary trial intervention involves on...   \n42  any patient eligible for the secondary trial w...   \n43  Patients must be female and have confirmed adv...   \n44  Women currently undergoing endocrine therapy a...   \n45  Between the secondary trial and the primary tr...   \n46  the primary trial measures the Percentage of P...   \n47  There more cases of Acute myocardial infarctio...   \n48  There were more cases of Eye disorders, Abdomi...   \n49  There is only one case of Diarrhea in the seco...   \n50  the primary trial and the secondary trial have...   \n51  All patients in the primary trial receive high...   \n52  There are more cases of Holocraneal cephale an...   \n53  Diarrhoea is the most common adverse event in ...   \n54  the primary trial and the secondary trial repo...   \n55  the secondary trial and the primary trial use ...   \n56  the results of the secondary trial and the pri...   \n57  The the primary trial intervention involves on...   \n58  the secondary trial reported 1 single case of ...   \n59  the secondary trial and the primary trial do n...   \n\n                                            evidences          label  \n0   [\"  Cardiac left ventricular function with res...  Contradiction  \n1   [\"  Enterocolitis 1/167 (0.60%)\", \"  Enterocol...  Contradiction  \n2   [\"Adverse Events 1:\", \"  Total: 17/65 (26.15%)...  Contradiction  \n3   [\"  Enterocolitis 1/167 (0.60%)\", \"  Enterocol...     Entailment  \n4   [\"  Presence of hot flashes for  30 days prior...  Contradiction  \n5   [\"Outcome Measurement: \", \"  Number of Patient...  Contradiction  \n6   [\"Outcome Measurement: \", \"  Overall Response ...  Contradiction  \n7   [\"Adverse Events 1:\", \"  Total: 0/23 (0.00%)\",...     Entailment  \n8   [\"Inclusion Criteria:\", \"  Female or male pati...     Entailment  \n9   [\"Adverse Events 1:\", \"  Total: 0/34 (0.00%)\",...  Contradiction  \n10  [\"INTERVENTION 1: \", \"  BKM120 and Paclitaxel\"...  Contradiction  \n11  [\"  Histologically or cytologically confirmed ...     Entailment  \n12  [\"Outcome Measurement: \", \"  Number of Partici...     Entailment  \n13  [\"INTERVENTION 1: \", \"  Intraductal Arm\", \"  P...     Entailment  \n14  [\"Adverse Events 1:\", \"  Urosepsis 2/12 (16.67...     Entailment  \n15  [\"INTERVENTION 1: \", \"  Positron Emission Mamm...     Entailment  \n16  [\"INTERVENTION 1: \", \"  Arm A - Flaxseed & Act...  Contradiction  \n17  [\"Adverse Events 1:\", \"  Total: 0/655 (0.00%)\"...     Entailment  \n18  [\"INTERVENTION 1: \", \"  Arm A - Flaxseed & Act...     Entailment  \n19  [\"INTERVENTION 1: \", \"  Positron Emission Mamm...  Contradiction  \n20  [\"Inclusion Criteria:\", \"  Females with histol...  Contradiction  \n21  [\"  Unit of Measure: percentage of participant...     Entailment  \n22  [\"Outcome Measurement: \", \"  Percentage of Par...     Entailment  \n23  [\"Outcome Measurement: \", \"  LDex Change-\", \" ...  Contradiction  \n24  [\"Outcome Measurement: \", \"  Objective Respons...  Contradiction  \n25  [\"Outcome Measurement: \", \"  LDex Change-\", \" ...     Entailment  \n26  [\"Adverse Events 1:\", \"  Total: 0/655 (0.00%)\"...  Contradiction  \n27  [\"Adverse Events 1:\", \"  Total: 0/0\", \"Adverse...  Contradiction  \n28  [\"Adverse Events 1:\", \"  Total: 16/48 (33.33%)...  Contradiction  \n29  [\"  Cardiac left ventricular function with res...     Entailment  \n30  [\"  Histologically or cytologically confirmed ...  Contradiction  \n31  [\"Outcome Measurement: \", \"  Overall Response ...     Entailment  \n32  [\"Exclusion Criteria:\", \"  Women previously di...  Contradiction  \n33  [\"INTERVENTION 1: \", \"  Intraductal Arm\", \"  P...  Contradiction  \n34  [\"Adverse Events 1:\", \"  Total: 0/23 (0.00%)\",...  Contradiction  \n35  [\"Outcome Measurement: \", \"  Safety of Externa...     Entailment  \n36  [\"Adverse Events 1:\", \"  Total: 0/150 (0.00%)\"...  Contradiction  \n37  [\"Outcome Measurement: \", \"  Number of Partici...  Contradiction  \n38  [\"Adverse Events 1:\", \"  Total: 59/373 (15.82%...     Entailment  \n39  [\"  Presence of hot flashes for  30 days prior...     Entailment  \n40  [\"Outcome Measurement: \", \"  Percent Change in...  Contradiction  \n41  [\"INTERVENTION 1: \", \"  Letrozole\", \"  Partici...     Entailment  \n42  [\"Inclusion Criteria:\", \"  Female or male pati...  Contradiction  \n43  [\"Inclusion Criteria:\", \"  Females with histol...     Entailment  \n44  [\"Exclusion Criteria:\", \"  Women previously di...     Entailment  \n45  [\"Adverse Events 1:\", \"  Total: 0/34 (0.00%)\",...     Entailment  \n46  [\"Outcome Measurement: \", \"  Percentage of Par...  Contradiction  \n47  [\"Adverse Events 1:\", \"  Total: 0/0\", \"Adverse...     Entailment  \n48  [\"Adverse Events 1:\", \"  Total: 0/150 (0.00%)\"...     Entailment  \n49  [\"Adverse Events 1:\", \"  Total: 59/373 (15.82%...  Contradiction  \n50  [\"Outcome Measurement: \", \"  Percent Change in...     Entailment  \n51  [\"INTERVENTION 1: \", \"  BKM120 and Paclitaxel\"...     Entailment  \n52  [\"Adverse Events 1:\", \"  Urosepsis 2/12 (16.67...  Contradiction  \n53  [\"Adverse Events 1:\", \"  Total: 17/65 (26.15%)...     Entailment  \n54  [\"  Unit of Measure: percentage of participant...  Contradiction  \n55  [\"Outcome Measurement: \", \"  Objective Respons...     Entailment  \n56  [\"Outcome Measurement: \", \"  Safety of Externa...  Contradiction  \n57  [\"INTERVENTION 1: \", \"  Letrozole\", \"  Partici...  Contradiction  \n58  [\"Adverse Events 1:\", \"  Total: 16/48 (33.33%)...     Entailment  \n59  [\"Outcome Measurement: \", \"  Number of Patient...     Entailment  \nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(comparison_data)):\n            batch_texts, batch_labels = comparison_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:29.510133Z","iopub.execute_input":"2024-04-28T18:33:29.510395Z","iopub.status.idle":"2024-04-28T18:33:45.238956Z","shell.execute_reply.started":"2024-04-28T18:33:29.510371Z","shell.execute_reply":"2024-04-28T18:33:45.238086Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:45.240053Z","iopub.execute_input":"2024-04-28T18:33:45.240335Z","iopub.status.idle":"2024-04-28T18:33:45.326090Z","shell.execute_reply.started":"2024-04-28T18:33:45.240309Z","shell.execute_reply":"2024-04-28T18:33:45.325148Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.519701  0.666667  0.500000  1.000000  0.500000   \n1            1  0.541419  0.666667  0.500000  1.000000  0.500000   \n2            2  0.502462  0.545455  0.500000  0.600000  0.500000   \n3            3  0.530888  0.666667  0.500000  1.000000  0.500000   \n4            4  0.595833  0.600000  0.525000  0.700000  0.533333   \n5            5  0.717048  0.666667  0.589744  0.766667  0.616667   \n6            6  0.743195  0.683544  0.551020  0.900000  0.583333   \n7            7  0.709807  0.566667  0.566667  0.566667  0.566667   \n8            8  0.748067  0.622951  0.612903  0.633333  0.616667   \n9            9  0.727379  0.565217  0.812500  0.433333  0.666667   \n10          10  0.715951  0.510638  0.705882  0.400000  0.616667   \n\n    Faithfulness  Consistency  \n0       0.500000     0.500000  \n1       0.500000     0.500000  \n2       0.500000     0.500000  \n3       0.500000     0.500000  \n4       0.466667     0.533333  \n5       0.383333     0.616667  \n6       0.416667     0.583333  \n7       0.433333     0.566667  \n8       0.383333     0.616667  \n9       0.333333     0.666667  \n10      0.383333     0.616667  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.519701</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.541419</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.502462</td>\n      <td>0.545455</td>\n      <td>0.500000</td>\n      <td>0.600000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.530888</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.595833</td>\n      <td>0.600000</td>\n      <td>0.525000</td>\n      <td>0.700000</td>\n      <td>0.533333</td>\n      <td>0.466667</td>\n      <td>0.533333</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.717048</td>\n      <td>0.666667</td>\n      <td>0.589744</td>\n      <td>0.766667</td>\n      <td>0.616667</td>\n      <td>0.383333</td>\n      <td>0.616667</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.743195</td>\n      <td>0.683544</td>\n      <td>0.551020</td>\n      <td>0.900000</td>\n      <td>0.583333</td>\n      <td>0.416667</td>\n      <td>0.583333</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.709807</td>\n      <td>0.566667</td>\n      <td>0.566667</td>\n      <td>0.566667</td>\n      <td>0.566667</td>\n      <td>0.433333</td>\n      <td>0.566667</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.748067</td>\n      <td>0.622951</td>\n      <td>0.612903</td>\n      <td>0.633333</td>\n      <td>0.616667</td>\n      <td>0.383333</td>\n      <td>0.616667</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.727379</td>\n      <td>0.565217</td>\n      <td>0.812500</td>\n      <td>0.433333</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.715951</td>\n      <td>0.510638</td>\n      <td>0.705882</td>\n      <td>0.400000</td>\n      <td>0.616667</td>\n      <td>0.383333</td>\n      <td>0.616667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for Comparison Type data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adverse_events_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/AdverseEvents_hypothesis_evidences.csv')\nprint(adverse_events_df)\n\nhypothesis_lst=adverse_events_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=adverse_events_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=adverse_events_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\nadverse_events_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:45.330763Z","iopub.execute_input":"2024-04-28T18:33:45.331070Z","iopub.status.idle":"2024-04-28T18:33:45.387730Z","shell.execute_reply.started":"2024-04-28T18:33:45.331044Z","shell.execute_reply":"2024-04-28T18:33:45.386767Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"                                           hypothesis  \\\n0   a significant number of the participants in th...   \n1   The only case of congestive heart failure in t...   \n2   Anaemia was the most common adverse event in c...   \n3   The adverse events section in the primary tria...   \n4   Diarrhoea is the most common adverse event in ...   \n5   most participants in the secondary trial and t...   \n6   The adverse events section in the primary tria...   \n7   there were 4 types of Adverse events which did...   \n8   None of the individual Aes recorded in the pri...   \n9   The most common adverse event in the primary t...   \n10  Febrile Neutropenia was the most common advers...   \n11  None of the individual Aes recorded in the pri...   \n12  the primary trial and the secondary trial do n...   \n13  Between the secondary trial and the primary tr...   \n14  Syncope was the most common adverse event reco...   \n15  no cardiac or neural adverse events were recor...   \n16  there was a higher percentage of patients with...   \n17  There are more cases of Urosepsis in the prima...   \n18  There were multiple patients which suffered Ne...   \n19  There adverse events section in the primary tr...   \n20  the primary trial and the secondary trial do n...   \n21  none of the individual adverse events in the p...   \n22  several cases of congestive heart failure in t...   \n23  in cohort 1 of the primary trial there were mo...   \n24  Infections were the most common adverse events...   \n25  the primary trial and the secondary trial do n...   \n26  There less cases of Acute myocardial infarctio...   \n27  the secondary trial reported 1 single case of ...   \n28  Anaemia was the most common adverse event in c...   \n29  There is no adverse events section in the prim...   \n30  there was a significantly higher percentage of...   \n31  in cohort 1 of the primary trial there were 10...   \n32  Only 6/67 patients in cohort 1 of the primary ...   \n33  none of the individual adverse events in the p...   \n34  the primary trial and the secondary trial repo...   \n35  no cardiac or bowel-related adverse events wer...   \n36  There were more cases of mental health disorde...   \n37  There is one case of Diarrhea in the secondary...   \n38  Between the secondary trial and the primary tr...   \n39  there were two types of Adverse events which d...   \n40  The least common adverse event in the primary ...   \n41  There more cases of Acute myocardial infarctio...   \n42  There were more total AEs in cohort 1 of  the ...   \n43  over 1/6 patients in cohort 1 of the primary t...   \n44  There were more cases of Eye disorders, Abdomi...   \n45  There is only one case of Diarrhea in the seco...   \n46  There are more cases of Holocraneal cephale an...   \n47  Febrile Neutropenia was the most common advers...   \n48  Diarrhoea is the most common adverse event in ...   \n49  There was one case of  Neutropenia in both coh...   \n50  The most common AE in the primary trial was No...   \n51  the secondary trial reported 1 single case of ...   \n\n                                            evidences          label  \n0   [\"  Enterocolitis 1/167 (0.60%)\", \"  Enterocol...  Contradiction  \n1   [\"Adverse Events 1:\", \"  Total: 1/35 (2.86%)\",...     Entailment  \n2   [\"Adverse Events 1:\", \"  Total: 59/199 (29.65%...     Entailment  \n3               [\"Adverse Events 1:\", \"  Total: 0/0\"]  Contradiction  \n4   [\"Adverse Events 1:\", \"  Total: 17/65 (26.15%)...  Contradiction  \n5   [\"  Enterocolitis 1/167 (0.60%)\", \"  Enterocol...     Entailment  \n6               [\"Adverse Events 1:\", \"  Total: 0/0\"]     Entailment  \n7   [\"Adverse Events 1:\", \"  Total: 4/65 (6.15%)\",...  Contradiction  \n8   [\"Adverse Events 1:\", \"  Total: 18/100 (18.00%...  Contradiction  \n9   [\"Adverse Events 1:\", \"  Total: 14/41 (34.15%)...     Entailment  \n10  [\"Adverse Events 1:\", \"  Total: 11/56 (19.64%)...  Contradiction  \n11  [\"Adverse Events 1:\", \"  Total: 18/100 (18.00%...     Entailment  \n12  [\"Adverse Events 1:\", \"  Total: 0/23 (0.00%)\",...     Entailment  \n13  [\"Adverse Events 1:\", \"  Total: 0/34 (0.00%)\",...  Contradiction  \n14  [\"Adverse Events 1:\", \"  Total: 5/55 (9.09%)\",...  Contradiction  \n15  [\"Adverse Events 1:\", \"  Total: 2/30 (6.67%)\",...     Entailment  \n16  [\"  SLIPPED DISK * 0/101 (0.00%)\", \"  SLIPPED ...     Entailment  \n17  [\"Adverse Events 1:\", \"  Urosepsis 2/12 (16.67...     Entailment  \n18  [\"Adverse Events 1:\", \"  Total: 3/39 (7.69%)\",...  Contradiction  \n19                [\"Adverse Events 1:\", \"  Total: 0\"]  Contradiction  \n20  [\"Adverse Events 1:\", \"  Total: 0/655 (0.00%)\"...     Entailment  \n21  [\"Adverse Events 1:\", \"  Total: 340/1612 (21.0...  Contradiction  \n22  [\"Adverse Events 1:\", \"  Total: 1/35 (2.86%)\",...  Contradiction  \n23  [\"  Left ventricular dysfunction * 2/219 (0.91...     Entailment  \n24  [\"Adverse Events 1:\", \"  Total: 5/55 (9.09%)\",...     Entailment  \n25  [\"Adverse Events 1:\", \"  Total: 0/655 (0.00%)\"...  Contradiction  \n26  [\"Adverse Events 1:\", \"  Total: 0/0\", \"Adverse...  Contradiction  \n27  [\"Adverse Events 1:\", \"  Total: 16/48 (33.33%)...  Contradiction  \n28  [\"Adverse Events 1:\", \"  Total: 59/199 (29.65%...  Contradiction  \n29                [\"Adverse Events 1:\", \"  Total: 0\"]     Entailment  \n30  [\"  SLIPPED DISK * 0/101 (0.00%)\", \"  SLIPPED ...  Contradiction  \n31  [\"  Left ventricular dysfunction * 2/219 (0.91...  Contradiction  \n32                [\"Adverse Events 1:\", \"  Total: 6\"]     Entailment  \n33  [\"Adverse Events 1:\", \"  Total: 340/1612 (21.0...     Entailment  \n34  [\"Adverse Events 1:\", \"  Total: 0/23 (0.00%)\",...  Contradiction  \n35  [\"Adverse Events 1:\", \"  Total: 2/30 (6.67%)\",...  Contradiction  \n36  [\"Adverse Events 1:\", \"  Total: 0/150 (0.00%)\"...  Contradiction  \n37  [\"Adverse Events 1:\", \"  Total: 59/373 (15.82%...     Entailment  \n38  [\"Adverse Events 1:\", \"  Total: 0/34 (0.00%)\",...     Entailment  \n39  [\"Adverse Events 1:\", \"  Total: 4/65 (6.15%)\",...     Entailment  \n40  [\"Adverse Events 1:\", \"  Total: 14/41 (34.15%)...  Contradiction  \n41  [\"Adverse Events 1:\", \"  Total: 0/0\", \"Adverse...     Entailment  \n42  [\"Adverse Events 1:\", \"  Total: 13/61 (21.31%)...  Contradiction  \n43                [\"Adverse Events 1:\", \"  Total: 6\"]  Contradiction  \n44  [\"Adverse Events 1:\", \"  Total: 0/150 (0.00%)\"...     Entailment  \n45  [\"Adverse Events 1:\", \"  Total: 59/373 (15.82%...  Contradiction  \n46  [\"Adverse Events 1:\", \"  Urosepsis 2/12 (16.67...  Contradiction  \n47  [\"Adverse Events 1:\", \"  Total: 11/56 (19.64%)...     Entailment  \n48  [\"Adverse Events 1:\", \"  Total: 17/65 (26.15%)...     Entailment  \n49  [\"Adverse Events 1:\", \"  Total: 3/39 (7.69%)\",...     Entailment  \n50  [\"Adverse Events 1:\", \"  Total: 13/61 (21.31%)...     Entailment  \n51  [\"Adverse Events 1:\", \"  Total: 16/48 (33.33%)...     Entailment  \nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(adverse_events_data)):\n            batch_texts, batch_labels = adverse_events_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:45.388832Z","iopub.execute_input":"2024-04-28T18:33:45.389132Z","iopub.status.idle":"2024-04-28T18:33:59.831575Z","shell.execute_reply.started":"2024-04-28T18:33:45.389105Z","shell.execute_reply":"2024-04-28T18:33:59.830755Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:59.832712Z","iopub.execute_input":"2024-04-28T18:33:59.833008Z","iopub.status.idle":"2024-04-28T18:33:59.915388Z","shell.execute_reply.started":"2024-04-28T18:33:59.832982Z","shell.execute_reply":"2024-04-28T18:33:59.914501Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.503390  0.666667  0.500000  1.000000  0.500000   \n1            1  0.629988  0.666667  0.500000  1.000000  0.500000   \n2            2  0.543418  0.509804  0.520000  0.500000  0.519231   \n3            3  0.579947  0.666667  0.500000  1.000000  0.500000   \n4            4  0.589782  0.625000  0.526316  0.769231  0.538462   \n5            5  0.676807  0.656250  0.552632  0.807692  0.576923   \n6            6  0.658677  0.657143  0.522727  0.884615  0.538462   \n7            7  0.656688  0.538462  0.538462  0.538462  0.538462   \n8            8  0.717157  0.646154  0.538462  0.807692  0.557692   \n9            9  0.702864  0.622222  0.736842  0.538462  0.673077   \n10          10  0.671303  0.577778  0.684211  0.500000  0.634615   \n\n    Faithfulness  Consistency  \n0       0.500000     0.500000  \n1       0.500000     0.500000  \n2       0.480769     0.519231  \n3       0.500000     0.500000  \n4       0.461538     0.538462  \n5       0.423077     0.576923  \n6       0.461538     0.538462  \n7       0.461538     0.538462  \n8       0.442308     0.557692  \n9       0.326923     0.673077  \n10      0.365385     0.634615  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.503390</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.629988</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.543418</td>\n      <td>0.509804</td>\n      <td>0.520000</td>\n      <td>0.500000</td>\n      <td>0.519231</td>\n      <td>0.480769</td>\n      <td>0.519231</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.579947</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.589782</td>\n      <td>0.625000</td>\n      <td>0.526316</td>\n      <td>0.769231</td>\n      <td>0.538462</td>\n      <td>0.461538</td>\n      <td>0.538462</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.676807</td>\n      <td>0.656250</td>\n      <td>0.552632</td>\n      <td>0.807692</td>\n      <td>0.576923</td>\n      <td>0.423077</td>\n      <td>0.576923</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.658677</td>\n      <td>0.657143</td>\n      <td>0.522727</td>\n      <td>0.884615</td>\n      <td>0.538462</td>\n      <td>0.461538</td>\n      <td>0.538462</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.656688</td>\n      <td>0.538462</td>\n      <td>0.538462</td>\n      <td>0.538462</td>\n      <td>0.538462</td>\n      <td>0.461538</td>\n      <td>0.538462</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.717157</td>\n      <td>0.646154</td>\n      <td>0.538462</td>\n      <td>0.807692</td>\n      <td>0.557692</td>\n      <td>0.442308</td>\n      <td>0.557692</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.702864</td>\n      <td>0.622222</td>\n      <td>0.736842</td>\n      <td>0.538462</td>\n      <td>0.673077</td>\n      <td>0.326923</td>\n      <td>0.673077</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.671303</td>\n      <td>0.577778</td>\n      <td>0.684211</td>\n      <td>0.500000</td>\n      <td>0.634615</td>\n      <td>0.365385</td>\n      <td>0.634615</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for Adverse events data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/Results_hypothesis_evidences.csv')\nprint(results_df)\n\nhypothesis_lst=results_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=results_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=results_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\nresults_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:59.916570Z","iopub.execute_input":"2024-04-28T18:33:59.916938Z","iopub.status.idle":"2024-04-28T18:33:59.986698Z","shell.execute_reply.started":"2024-04-28T18:33:59.916905Z","shell.execute_reply":"2024-04-28T18:33:59.985831Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"                                           hypothesis  \\\n0   there is a 13.2% difference between the result...   \n1   the primary trial does not report the PFS or o...   \n2   the shortest PFS in cohort 1 of the primary tr...   \n3   the outcome measurement of the primary trial i...   \n4   The the primary trial results section reports ...   \n5   The the primary trial results section reports ...   \n6   The shortest PFS in cohort 1 of the primary tr...   \n7   the secondary trial and the primary trial both...   \n8   there is a minimal difference between the resu...   \n9   the primary trial and the secondary trial use ...   \n10  at least 2 participants of the primary trial, ...   \n11  the primary trial does not report the PFS or o...   \n12  The results from the secondary trial and the p...   \n13  neither of the interventions in the primary tr...   \n14  The shortest PFS in cohort 1 of the primary tr...   \n15  there were 6 recorded deaths in cohort 1 of th...   \n16  the primary trial reports the changes in the n...   \n17  All patients in cohort 1 of the primary trial ...   \n18  the majority of participants in cohort 1 and 2...   \n19  the majority of participants in cohort 1 and 2...   \n20  Most patients in the primary trial experienced...   \n21  the primary trial and the secondary trial repo...   \n22  the primary trial reports the percentage of pa...   \n23  the primary trial measures the Percentage of P...   \n24  neither the secondary trial or the primary tri...   \n25  the secondary trial and the primary trial use ...   \n26  a higher proportion of patients suffered Disea...   \n27  the secondary trial and the primary trial do n...   \n28  the primary trial and the secondary trial use ...   \n29  the primary trial reports the Severity of Derm...   \n30  All patients in cohort 1 of the primary trial ...   \n31  the primary trial reports the Severity of Derm...   \n32  in the primary trial, patients that received F...   \n33  50% of cohort 2 patients in the primary trial ...   \n34  under a quater of the primary trial patients i...   \n35  the results of the secondary trial and the pri...   \n36  The results from the secondary trial and the p...   \n37  the primary trial results show that the Trastu...   \n38  there were no recorded deaths in the primary t...   \n39  the primary trial and the secondary trial meas...   \n40  50% of cohort 2 patients in the primary trial ...   \n41  the primary trial reports the percentage of pa...   \n42  under 30% of the primary trial patients in the...   \n43  the primary trial measures the Percentage of P...   \n44  the primary trial reports the changes in tumor...   \n45  the primary trial and the secondary trial have...   \n46  neither of the interventions in the primary tr...   \n47  the primary trial and the secondary trial repo...   \n48  Most patients in the primary trial experienced...   \n49  All the primary trial patients achieved either...   \n50  the secondary trial and the primary trial use ...   \n51  the results of the secondary trial and the pri...   \n52  the primary trial results show that the Trastu...   \n53  the secondary trial and the primary trial do n...   \n54  the outcome measurement of the primary trial i...   \n55  All the primary trial patients had a minimum o...   \n\n                                            evidences          label  \n0   [\"Outcome Measurement: \", \"  Event-free Surviv...  Contradiction  \n1   [\"Outcome Measurement: \", \"  Local Control Usi...     Entailment  \n2   [\"Outcome Measurement: \", \"  Progression-Free ...  Contradiction  \n3   [\"Outcome Measurement: \", \"  Progression-free ...  Contradiction  \n4   [\"Outcome Measurement: \", \"  Proportion of Sen...     Entailment  \n5   [\"Outcome Measurement: \", \"  Proportion of Sen...  Contradiction  \n6   [\"Outcome Measurement: \", \"  Progression Free ...  Contradiction  \n7   [\"Outcome Measurement: \", \"  Number of Patient...  Contradiction  \n8   [\"Outcome Measurement: \", \"  Event-free Surviv...     Entailment  \n9   [\"Outcome Measurement: \", \"  Overall Response ...  Contradiction  \n10  [\"Outcome Measurement: \", \"  Progression-Free ...     Entailment  \n11  [\"Outcome Measurement: \", \"  Local Control Usi...  Contradiction  \n12  [\"Outcome Measurement: \", \"  Number of Partici...     Entailment  \n13  [\"Outcome Measurement: \", \"  Overall Response ...     Entailment  \n14  [\"Outcome Measurement: \", \"  Progression Free ...     Entailment  \n15  [\"Outcome Measurement: \", \"  Patients Event-fr...  Contradiction  \n16  [\"Outcome Measurement: \", \"  Change in Tumor S...  Contradiction  \n17  [\"Outcome Measurement: \", \"  Centrally Assesse...     Entailment  \n18  [\"Outcome Measurement: \", \"  Disease-free Surv...     Entailment  \n19  [\"Outcome Measurement: \", \"  Disease-free Surv...  Contradiction  \n20  [\"Outcome Measurement: \", \"  Severity of Adver...     Entailment  \n21  [\"  Unit of Measure: percentage of participant...     Entailment  \n22  [\"Outcome Measurement: \", \"  Dose Limiting Tox...     Entailment  \n23  [\"Outcome Measurement: \", \"  Percentage of Par...     Entailment  \n24  [\"Outcome Measurement: \", \"  LDex Change-\", \" ...  Contradiction  \n25  [\"Outcome Measurement: \", \"  Objective Respons...  Contradiction  \n26  [\"Outcome Measurement: \", \"  Number of Partici...     Entailment  \n27  [\"Outcome Measurement: \", \"  LDex Change-\", \" ...     Entailment  \n28  [\"Outcome Measurement: \", \"  Overall Response ...     Entailment  \n29  [\"Outcome Measurement: \", \"  Severity of Derma...     Entailment  \n30  [\"Outcome Measurement: \", \"  Centrally Assesse...  Contradiction  \n31  [\"Outcome Measurement: \", \"  Severity of Derma...  Contradiction  \n32  [\"Outcome Measurement: \", \"  Number of Partici...  Contradiction  \n33  [\"Results 2: \", \"  Arm/Group Title: AZD8931 12...     Entailment  \n34  [\"Outcome Measurement: \", \"  Pathologic Comple...  Contradiction  \n35  [\"Outcome Measurement: \", \"  Safety of Externa...     Entailment  \n36  [\"Outcome Measurement: \", \"  Number of Partici...  Contradiction  \n37  [\"Outcome Measurement: \", \"  Number of Partici...     Entailment  \n38  [\"Outcome Measurement: \", \"  Patients Event-fr...     Entailment  \n39  [\"Outcome Measurement: \", \"  Percent Change in...  Contradiction  \n40  [\"Results 2: \", \"  Arm/Group Title: AZD8931 12...  Contradiction  \n41  [\"Outcome Measurement: \", \"  Dose Limiting Tox...  Contradiction  \n42  [\"Outcome Measurement: \", \"  Pathologic Comple...     Entailment  \n43  [\"Outcome Measurement: \", \"  Percentage of Par...  Contradiction  \n44  [\"Outcome Measurement: \", \"  Change in Tumor S...     Entailment  \n45  [\"Outcome Measurement: \", \"  Percent Change in...     Entailment  \n46  [\"Outcome Measurement: \", \"  Overall Response ...  Contradiction  \n47  [\"  Unit of Measure: percentage of participant...  Contradiction  \n48  [\"Outcome Measurement: \", \"  Severity of Adver...  Contradiction  \n49  [\"Outcome Measurement: \", \"  Complete Response...     Entailment  \n50  [\"Outcome Measurement: \", \"  Objective Respons...     Entailment  \n51  [\"Outcome Measurement: \", \"  Safety of Externa...  Contradiction  \n52  [\"Outcome Measurement: \", \"  Number of Partici...  Contradiction  \n53  [\"Outcome Measurement: \", \"  Number of Patient...     Entailment  \n54  [\"Outcome Measurement: \", \"  Progression-free ...     Entailment  \n55  [\"Outcome Measurement: \", \"  Complete Response...  Contradiction  \nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(results_data)):\n            batch_texts, batch_labels = results_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:33:59.987885Z","iopub.execute_input":"2024-04-28T18:33:59.988221Z","iopub.status.idle":"2024-04-28T18:34:14.944231Z","shell.execute_reply.started":"2024-04-28T18:33:59.988187Z","shell.execute_reply":"2024-04-28T18:34:14.943423Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:34:14.945448Z","iopub.execute_input":"2024-04-28T18:34:14.945768Z","iopub.status.idle":"2024-04-28T18:34:15.030602Z","shell.execute_reply.started":"2024-04-28T18:34:14.945741Z","shell.execute_reply":"2024-04-28T18:34:15.029680Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.529943  0.666667  0.520000  0.928571  0.535714   \n1            1  0.492687  0.666667  0.500000  1.000000  0.500000   \n2            2  0.516032  0.579710  0.487805  0.714286  0.482143   \n3            3  0.517777  0.666667  0.500000  1.000000  0.500000   \n4            4  0.548776  0.575758  0.500000  0.678571  0.500000   \n5            5  0.656860  0.597015  0.512821  0.714286  0.517857   \n6            6  0.687813  0.657534  0.533333  0.857143  0.553571   \n7            7  0.705771  0.603175  0.542857  0.678571  0.553571   \n8            8  0.740818  0.646154  0.567568  0.750000  0.589286   \n9            9  0.733778  0.565217  0.722222  0.464286  0.642857   \n10          10  0.723682  0.523810  0.785714  0.392857  0.642857   \n\n    Faithfulness  Consistency  \n0       0.464286     0.535714  \n1       0.500000     0.500000  \n2       0.517857     0.482143  \n3       0.500000     0.500000  \n4       0.500000     0.500000  \n5       0.482143     0.517857  \n6       0.446429     0.553571  \n7       0.446429     0.553571  \n8       0.410714     0.589286  \n9       0.357143     0.642857  \n10      0.357143     0.642857  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.529943</td>\n      <td>0.666667</td>\n      <td>0.520000</td>\n      <td>0.928571</td>\n      <td>0.535714</td>\n      <td>0.464286</td>\n      <td>0.535714</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.492687</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.516032</td>\n      <td>0.579710</td>\n      <td>0.487805</td>\n      <td>0.714286</td>\n      <td>0.482143</td>\n      <td>0.517857</td>\n      <td>0.482143</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.517777</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.548776</td>\n      <td>0.575758</td>\n      <td>0.500000</td>\n      <td>0.678571</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.656860</td>\n      <td>0.597015</td>\n      <td>0.512821</td>\n      <td>0.714286</td>\n      <td>0.517857</td>\n      <td>0.482143</td>\n      <td>0.517857</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.687813</td>\n      <td>0.657534</td>\n      <td>0.533333</td>\n      <td>0.857143</td>\n      <td>0.553571</td>\n      <td>0.446429</td>\n      <td>0.553571</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.705771</td>\n      <td>0.603175</td>\n      <td>0.542857</td>\n      <td>0.678571</td>\n      <td>0.553571</td>\n      <td>0.446429</td>\n      <td>0.553571</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.740818</td>\n      <td>0.646154</td>\n      <td>0.567568</td>\n      <td>0.750000</td>\n      <td>0.589286</td>\n      <td>0.410714</td>\n      <td>0.589286</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.733778</td>\n      <td>0.565217</td>\n      <td>0.722222</td>\n      <td>0.464286</td>\n      <td>0.642857</td>\n      <td>0.357143</td>\n      <td>0.642857</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.723682</td>\n      <td>0.523810</td>\n      <td>0.785714</td>\n      <td>0.392857</td>\n      <td>0.642857</td>\n      <td>0.357143</td>\n      <td>0.642857</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for Results data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eligibility_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/Eligibility_hypothesis_evidences.csv')\nprint(eligibility_df)\n\nhypothesis_lst=eligibility_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=eligibility_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=eligibility_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\neligibility_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:34:15.032020Z","iopub.execute_input":"2024-04-28T18:34:15.032775Z","iopub.status.idle":"2024-04-28T18:34:15.100886Z","shell.execute_reply.started":"2024-04-28T18:34:15.032738Z","shell.execute_reply":"2024-04-28T18:34:15.100008Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"                                           hypothesis  \\\n0   Patients with significantly elevated ejection ...   \n1   Prior treatment with fulvestrant or with a pho...   \n2   only patients with a HER2-positive status can ...   \n3   T4 N2 M4 patients are eligible for the primary...   \n4   Patients must have a palpable carcinoma to be ...   \n5   to be eligible for the primary trial patients ...   \n6   ongoing flucytosine treatments are not permitt...   \n7   Only patients with triple negative breast canc...   \n8   adults and children can take part in the prima...   \n9   Prior treatment with fulvestrant or with a pho...   \n10  Presence of hot flashes for at least 4 month p...   \n11  to be eligible for the primary trial candidate...   \n12  A patient with a node positive T2 N2 M0 adenoc...   \n13  ECOG score < 2 is necessary to be eligible for...   \n14  patients with a Malignant brain tumour or colo...   \n15  the secondary trial and the primary trial do n...   \n16  all participants of the primary trial have sta...   \n17  to be eligible for the primary trial patients ...   \n18  all pregnant patients accepted into the primar...   \n19     Only adults can take part in the primary trial   \n20  Patients with Extracranial metastases can be e...   \n21  patients with a Malignant brain tumour diagnos...   \n22  ongoing flucytosine treatments are necessary f...   \n23  Patients must be able to undergo a PET scan to...   \n24  a patient with a HER2-positive status can not ...   \n25  Breast cancer patients must have documented st...   \n26  Patients wanting to take part in the primary t...   \n27  Patients must be female and have confirmed adv...   \n28  Patients wanting to take part in the primary t...   \n29  eliane is a 56 year old woman, and elizabeth i...   \n30  Breast cancer patients must have documented pr...   \n31  Patients with significantly low ejection fract...   \n32  all participants of the primary trial have sta...   \n33  Patients with stage 4 cancer are excluded from...   \n34  Patients must have a Life expectancy of at lea...   \n35  Women currently undergoing endocrine therapy a...   \n36  Patients with stage 4 cancer are excluded from...   \n37  to be eligible for the primary trial candidate...   \n38  patients must be able to receive medication or...   \n39  T1 N0 M0 patients are eligible for the primary...   \n40  Patients with Histologically confirmed breast ...   \n41  patients must have HER2- breast cancer to part...   \n42  Presence of hot flashes for  30 days prior to ...   \n43  Patients with Histologically confirmed breast ...   \n44  Only patients with HER2+ve breast cancer are e...   \n45  any patient eligible for the secondary trial w...   \n46  ECOG score > 2 is necessary to be eligible for...   \n47  Patients must have several visible carcinomas ...   \n48  Patients must be female and have confirmed adv...   \n49  Women currently undergoing endocrine therapy a...   \n50  Patients must have a Life expectancy of 75+ ye...   \n51  eliane is a 56 year old woman, and alex is a 3...   \n52  all patients accepted into the primary trial h...   \n53  Patients must have no radiographically confirm...   \n54  A patient with a node positive T2 N2 M0 adenoc...   \n55  Patients must be bedbound or severaly disabled...   \n\n                                            evidences          label  \n0   [\"  Cardiac left ventricular function with res...  Contradiction  \n1   [\"  Prior treatment with a phosphatidylinosito...  Contradiction  \n2   [\"  HER2-positive status (patients who have un...  Contradiction  \n3                                [\"  T1-3, N0-2, M0\"]  Contradiction  \n4   [\"  Histologic diagnosis of palpable invasive ...     Entailment  \n5   [\"Inclusion Criteria:\", \"  Confirmed hormone r...     Entailment  \n6                [\"  Being treated with flucytosine\"]     Entailment  \n7   [\"  HER-2 positive BC as defined by an immunoh...  Contradiction  \n8                                 [\"  Age  18 years\"]  Contradiction  \n9   [\"  Prior treatment with a phosphatidylinosito...     Entailment  \n10  [\"  Presence of hot flashes for  30 days prior...  Contradiction  \n11  [\"  Age between 18 years and 70 years.\", \"  Ka...  Contradiction  \n12  [\"  node-positive: T1-3, N1-2, M0 (level of T ...     Entailment  \n13  [\"  Eastern Cooperative Oncology Group (ECOG) ...     Entailment  \n14         [\"  Cancer that has spread to the brain.\"]  Contradiction  \n15  [\"Inclusion Criteria:\", \"  Female or male pati...     Entailment  \n16  [\"  Histologically or cytologically confirmed ...     Entailment  \n17  [\"Inclusion Criteria:\", \"  Confirmed hormone r...  Contradiction  \n18  [\"  Documentation of negative pregnancy test.\"...  Contradiction  \n19                                [\"  Age  18 years\"]     Entailment  \n20              [\"  Extracranial metastases allowed\"]     Entailment  \n21         [\"  Cancer that has spread to the brain.\"]     Entailment  \n22               [\"  Being treated with flucytosine\"]  Contradiction  \n23  [\"  Able to lie still for 1.5 hours for PET sc...     Entailment  \n24  [\"  HER2-positive status (patients who have un...     Entailment  \n25  [\"  Pathologically confirmed diagnosis of brea...  Contradiction  \n26  [\"  Known and documented HER2-positive\", \"  Kn...     Entailment  \n27  [\"Inclusion Criteria:\", \"  Females with histol...  Contradiction  \n28  [\"  Known and documented HER2-positive\", \"  Kn...  Contradiction  \n29  [\"Inclusion Criteria:\", \"  women >=18 years of...     Entailment  \n30  [\"  Pathologically confirmed diagnosis of brea...     Entailment  \n31  [\"  Cardiac left ventricular function with res...     Entailment  \n32  [\"  Histologically or cytologically confirmed ...  Contradiction  \n33  [\"Eligibility Criteria:\", \"  Newly diagnosed w...  Contradiction  \n34                     [\"  Life expectancy  1 year.\"]     Entailment  \n35  [\"Exclusion Criteria:\", \"  Women previously di...  Contradiction  \n36  [\"Eligibility Criteria:\", \"  Newly diagnosed w...     Entailment  \n37  [\"  Age between 18 years and 70 years.\", \"  Ka...     Entailment  \n38          [\"  Able to swallow an oral medication.\"]     Entailment  \n39                               [\"  T1-3, N0-2, M0\"]     Entailment  \n40  [\"DISEASE CHARACTERISTICS:\", \"  Histologically...     Entailment  \n41     [\"  ErbB2(HER2)overexpressing breast cancer.\"]  Contradiction  \n42  [\"  Presence of hot flashes for  30 days prior...     Entailment  \n43  [\"DISEASE CHARACTERISTICS:\", \"  Histologically...  Contradiction  \n44  [\"  HER-2 positive BC as defined by an immunoh...     Entailment  \n45  [\"Inclusion Criteria:\", \"  Female or male pati...  Contradiction  \n46  [\"  Eastern Cooperative Oncology Group (ECOG) ...  Contradiction  \n47  [\"  Histologic diagnosis of palpable invasive ...  Contradiction  \n48  [\"Inclusion Criteria:\", \"  Females with histol...     Entailment  \n49  [\"Exclusion Criteria:\", \"  Women previously di...     Entailment  \n50                     [\"  Life expectancy  1 year.\"]  Contradiction  \n51  [\"Inclusion Criteria:\", \"  women >=18 years of...  Contradiction  \n52  [\"  Prior treatment for breast cancer with end...     Entailment  \n53  [\"  Histologically or cytologically confirmed ...  Contradiction  \n54  [\"  node-positive: T1-3, N1-2, M0 (level of T ...  Contradiction  \n55  [\"Inclusion Criteria:\", \"  Pathologically conf...  Contradiction  \nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(eligibility_data)):\n            batch_texts, batch_labels = eligibility_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:34:15.102293Z","iopub.execute_input":"2024-04-28T18:34:15.103049Z","iopub.status.idle":"2024-04-28T18:34:30.351125Z","shell.execute_reply.started":"2024-04-28T18:34:15.103014Z","shell.execute_reply":"2024-04-28T18:34:30.350029Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:34:30.352547Z","iopub.execute_input":"2024-04-28T18:34:30.353000Z","iopub.status.idle":"2024-04-28T18:34:30.446421Z","shell.execute_reply.started":"2024-04-28T18:34:30.352960Z","shell.execute_reply":"2024-04-28T18:34:30.445432Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.499670  0.000000  0.000000  0.000000  0.500000   \n1            1  0.504530  0.666667  0.500000  1.000000  0.500000   \n2            2  0.607497  0.526316  0.517241  0.535714  0.517857   \n3            3  0.574442  0.666667  0.500000  1.000000  0.500000   \n4            4  0.558770  0.658228  0.509804  0.928571  0.517857   \n5            5  0.642515  0.683544  0.529412  0.964286  0.553571   \n6            6  0.692479  0.658537  0.500000  0.964286  0.500000   \n7            7  0.702168  0.626866  0.538462  0.750000  0.553571   \n8            8  0.672882  0.647887  0.534884  0.821429  0.553571   \n9            9  0.653382  0.476190  0.714286  0.357143  0.607143   \n10          10  0.638285  0.553191  0.684211  0.464286  0.625000   \n\n    Faithfulness  Consistency  \n0       0.500000     0.500000  \n1       0.500000     0.500000  \n2       0.482143     0.517857  \n3       0.500000     0.500000  \n4       0.482143     0.517857  \n5       0.446429     0.553571  \n6       0.500000     0.500000  \n7       0.446429     0.553571  \n8       0.446429     0.553571  \n9       0.392857     0.607143  \n10      0.375000     0.625000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.499670</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.504530</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.607497</td>\n      <td>0.526316</td>\n      <td>0.517241</td>\n      <td>0.535714</td>\n      <td>0.517857</td>\n      <td>0.482143</td>\n      <td>0.517857</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.574442</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.558770</td>\n      <td>0.658228</td>\n      <td>0.509804</td>\n      <td>0.928571</td>\n      <td>0.517857</td>\n      <td>0.482143</td>\n      <td>0.517857</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.642515</td>\n      <td>0.683544</td>\n      <td>0.529412</td>\n      <td>0.964286</td>\n      <td>0.553571</td>\n      <td>0.446429</td>\n      <td>0.553571</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.692479</td>\n      <td>0.658537</td>\n      <td>0.500000</td>\n      <td>0.964286</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.702168</td>\n      <td>0.626866</td>\n      <td>0.538462</td>\n      <td>0.750000</td>\n      <td>0.553571</td>\n      <td>0.446429</td>\n      <td>0.553571</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.672882</td>\n      <td>0.647887</td>\n      <td>0.534884</td>\n      <td>0.821429</td>\n      <td>0.553571</td>\n      <td>0.446429</td>\n      <td>0.553571</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.653382</td>\n      <td>0.476190</td>\n      <td>0.714286</td>\n      <td>0.357143</td>\n      <td>0.607143</td>\n      <td>0.392857</td>\n      <td>0.607143</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.638285</td>\n      <td>0.553191</td>\n      <td>0.684211</td>\n      <td>0.464286</td>\n      <td>0.625000</td>\n      <td>0.375000</td>\n      <td>0.625000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for Eligibility data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intervention_df=pd.read_csv('/kaggle/input/nlp-project/Training Data csv/Intervention_hypothesis_evidences.csv')\nprint(intervention_df)\n\nhypothesis_lst=intervention_df['hypothesis'].values.tolist()\nlen(hypothesis_lst)\n\nevidence_lst=intervention_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\nlen(evidence_lst)\n\nlabel2id={\"Contradiction\":0,\"Entailment\":1}\nlabel_lst=intervention_df['label'].apply(lambda x:label2id[x]).values.tolist()\nlen(label_lst)\n\nintervention_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:34:30.447660Z","iopub.execute_input":"2024-04-28T18:34:30.447978Z","iopub.status.idle":"2024-04-28T18:34:30.495309Z","shell.execute_reply.started":"2024-04-28T18:34:30.447950Z","shell.execute_reply":"2024-04-28T18:34:30.494427Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"                                           hypothesis  \\\n0   Both cohorts of the primary trial undergo a to...   \n1   both cohorts of the primary trial receive iden...   \n2   the primary trial participants recieve doses o...   \n3   Cohort 1 and 2 of the primary trial do not rec...   \n4   Participants of the primary trial will not rec...   \n5   the primary trial participants receive more th...   \n6   All patients in the primary trial receive high...   \n7   the primary trial participants are administere...   \n8   the primary trial and the secondary trial part...   \n9   Cohort 1 and 2 of the primary trial do not rec...   \n10  the primary trial participants are not adminis...   \n11  All the primary trial participants are adminis...   \n12  the primary trial participants are administere...   \n13  All the primary trial participants are adminis...   \n14  the primary trial participants are administere...   \n15  80% of patients in the primary trial received ...   \n16  the primary trial participants receive signifi...   \n17  the primary trial participants are administere...   \n18  Both cohorts of the primary trial undergo a to...   \n19  the primary trial and the secondary trial part...   \n20  both the primary trial cohorts receive identic...   \n21  neither of the cohorts in the primary trial ar...   \n22  the primary trial participants recieve less th...   \n23  Only cohort 1 patients of the primary trial ar...   \n24  One patient cohort in the primary trial receiv...   \n25  The the primary trial intervention involves on...   \n26  cohort 1 patients of the primary trial are adm...   \n27  the primary trial intervention protocol will c...   \n28  both the primary trial cohorts receive identic...   \n29  both cohorts of the primary trial receive iden...   \n30  All patients in the primary trial receive high...   \n31  neither of the cohorts in the primary trial ar...   \n32  the primary trial participants receive Capecit...   \n33  the primary trial intervention protocol lasts ...   \n34  Participants of the primary trial will receive...   \n35  The the primary trial intervention involves on...   \n\n                                            evidences          label  \n0   [\"INTERVENTION 1: \", \"  HER2+ TC\", \"  Particip...     Entailment  \n1   [\"INTERVENTION 1: \", \"  Nivolumab + Daratumuma...     Entailment  \n2   [\"INTERVENTION 1: \", \"  Pralatrexate\", \"  Stud...  Contradiction  \n3   [\"INTERVENTION 1: \", \"  NKTR-102\", \"  In Group...  Contradiction  \n4   [\"INTERVENTION 1: \", \"  Abraxane, Avastin and ...     Entailment  \n5   [\"INTERVENTION 1: \", \"  Avastin (Bevacizumab) ...     Entailment  \n6   [\"INTERVENTION 1: \", \"  BKM120 and Paclitaxel\"...  Contradiction  \n7   [\"INTERVENTION 1: \", \"  Fulvestrant + Everolim...  Contradiction  \n8   [\"INTERVENTION 1: \", \"  Intraductal Arm\", \"  P...     Entailment  \n9   [\"INTERVENTION 1: \", \"  NKTR-102\", \"  In Group...     Entailment  \n10  [\"INTERVENTION 1: \", \"  Positron Emission Mamm...     Entailment  \n11  [\"INTERVENTION 1: \", \"  Arm A - Flaxseed & Act...  Contradiction  \n12  [\"INTERVENTION 1: \", \"  Avastin (Bevacizumab) ...  Contradiction  \n13  [\"INTERVENTION 1: \", \"  Arm A - Flaxseed & Act...     Entailment  \n14  [\"INTERVENTION 1: \", \"  Positron Emission Mamm...  Contradiction  \n15  [\"INTERVENTION 1: \", \"  Lapatinib 1500 mg\", \" ...  Contradiction  \n16  [\"INTERVENTION 1: \", \"  Lapatinib 1250 mg and ...     Entailment  \n17  [\"INTERVENTION 1: \", \"  Fulvestrant + Everolim...     Entailment  \n18  [\"INTERVENTION 1: \", \"  HER2+ TC\", \"  Particip...  Contradiction  \n19  [\"INTERVENTION 1: \", \"  Intraductal Arm\", \"  P...  Contradiction  \n20  [\"INTERVENTION 1: \", \"  Docetaxel 100 mg/m^2 P...     Entailment  \n21  [\"INTERVENTION 1: \", \"  No Exercise\", \"  Multi...  Contradiction  \n22  [\"INTERVENTION 1: \", \"  Pralatrexate\", \"  Stud...     Entailment  \n23  [\"INTERVENTION 1: \", \"  Ipatasertib + Paclitax...     Entailment  \n24  [\"INTERVENTION 1: \", \"  Lapatinib 1500 mg\", \" ...     Entailment  \n25  [\"INTERVENTION 1: \", \"  Letrozole\", \"  Partici...     Entailment  \n26  [\"INTERVENTION 1: \", \"  Ipatasertib + Paclitax...  Contradiction  \n27  [\"INTERVENTION 1: \", \"  Pemetrexed\", \"  600 mg...     Entailment  \n28  [\"INTERVENTION 1: \", \"  Docetaxel 100 mg/m^2 P...  Contradiction  \n29  [\"INTERVENTION 1: \", \"  Nivolumab + Daratumuma...  Contradiction  \n30  [\"INTERVENTION 1: \", \"  BKM120 and Paclitaxel\"...     Entailment  \n31  [\"INTERVENTION 1: \", \"  No Exercise\", \"  Multi...     Entailment  \n32  [\"INTERVENTION 1: \", \"  Lapatinib 1250 mg and ...  Contradiction  \n33  [\"INTERVENTION 1: \", \"  Pemetrexed\", \"  600 mg...  Contradiction  \n34  [\"INTERVENTION 1: \", \"  Abraxane, Avastin and ...  Contradiction  \n35  [\"INTERVENTION 1: \", \"  Letrozole\", \"  Partici...  Contradiction  \nTokenization done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\nimport pandas as pd\n\n# Define the faithfulness function\ndef faithfulness(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] != gold[key][\"Causal_type\"][1]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\n# Define the consistency function\ndef consistency(predictions, gold):\n    results = []\n    for key in predictions.keys():\n        # Adjust according to the actual structure of your data\n        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n            results.append(1)\n        else:\n            results.append(0)\n    return sum(results) / len(predictions)\n\nscores = []\nmodel_names = ['dmis-lab/biobert-base-cased-v1.2'] + [\n    '/kaggle/working/clf_models/dmis-lab/biobert-base-cased-v1.2_epoch_{}.pt'.format(format(epoch, '05d')) for epoch in range(10)\n]\n\n# Initialize containers for faithfulness and consistency scores\nfaithfulness_scores = []\nconsistency_scores = []\n\nfor model_name in model_names:\n    scores.append([])\n    clf = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\n    # Placeholder for storing predictions in the required format for faithfulness and consistency evaluations\n    predictions = {}\n    gold = {idx: {\"Label\": label, \"Causal_type\": [\"\", label]} for idx, label in enumerate(label_lst)}\n\n\n    with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n        for batch in range(len(intervention_data)):\n            batch_texts, batch_labels = intervention_data[batch]\n            batch_size = 32\n            num_batches = (len(batch_texts['input_ids']) + batch_size - 1) // batch_size\n\n            batch_logits = []\n            for i in range(num_batches):\n                start = i * batch_size\n                end = (i + 1) * batch_size\n                input_ids = batch_texts['input_ids'][start:end]\n                attention_mask = batch_texts['attention_mask'][start:end]\n                logits = clf(input_ids=input_ids, attention_mask=attention_mask).logits\n                batch_logits.append(logits.cpu())\n\n            logits = torch.cat(batch_logits, dim=0)\n            scores[-1].append(F.softmax(logits, dim=1).numpy())\n\n            # Simulate prediction storage for faithfulness and consistency evaluation\n            # This needs to be adapted to match your actual data structure\n            for idx, logit in enumerate(logits):\n                pred_label = torch.argmax(logit).item()\n                predictions[batch * batch_size + idx] = {\"Prediction\": pred_label}\n                # gold should be populated accordingly\n\n    scores[-1] = np.concatenate(scores[-1], axis=0)\n    clf.cpu()\n\n    # Calculate faithfulness and consistency here\n    faithfulness_scores.append(faithfulness(predictions, gold))\n    consistency_scores.append(consistency(predictions, gold))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:34:30.496559Z","iopub.execute_input":"2024-04-28T18:34:30.496939Z","iopub.status.idle":"2024-04-28T18:34:41.699002Z","shell.execute_reply.started":"2024-04-28T18:34:30.496896Z","shell.execute_reply":"2024-04-28T18:34:41.698146Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"y_true = label_lst\nresults = []\nfor epoch in range(len(scores)):\n    y_prob = scores[epoch][:, 1]\n    y_pred = [1 if a > 0.5 else 0 for a in y_prob]\n    results.append([\n        'pretrained' if epoch == 0 else epoch,\n        average_precision_score(y_true, y_prob),\n        f1_score(y_true, y_pred),\n        precision_score(y_true, y_pred),\n        recall_score(y_true, y_pred),\n        accuracy_score(y_true, y_pred),\n        faithfulness_scores[epoch],  # Add faithfulness score\n        consistency_scores[epoch]    # Add consistency score\n    ])\n\ncolumns = ['epoch', 'AVG_PREC', 'F1', 'PREC', 'REC', 'ACC', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T18:34:41.700192Z","iopub.execute_input":"2024-04-28T18:34:41.700483Z","iopub.status.idle":"2024-04-28T18:34:41.789062Z","shell.execute_reply.started":"2024-04-28T18:34:41.700456Z","shell.execute_reply":"2024-04-28T18:34:41.787907Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"         epoch  AVG_PREC        F1      PREC       REC       ACC  \\\n0   pretrained  0.521286  0.638298  0.517241  0.833333  0.527778   \n1            1  0.589758  0.666667  0.500000  1.000000  0.500000   \n2            2  0.530432  0.500000  0.500000  0.500000  0.500000   \n3            3  0.599219  0.666667  0.500000  1.000000  0.500000   \n4            4  0.574563  0.680000  0.531250  0.944444  0.555556   \n5            5  0.571859  0.636364  0.538462  0.777778  0.555556   \n6            6  0.613684  0.666667  0.533333  0.888889  0.555556   \n7            7  0.638155  0.711111  0.592593  0.888889  0.638889   \n8            8  0.679813  0.666667  0.555556  0.833333  0.583333   \n9            9  0.664490  0.516129  0.615385  0.444444  0.583333   \n10          10  0.617817  0.413793  0.545455  0.333333  0.527778   \n\n    Faithfulness  Consistency  \n0       0.472222     0.527778  \n1       0.500000     0.500000  \n2       0.500000     0.500000  \n3       0.500000     0.500000  \n4       0.444444     0.555556  \n5       0.444444     0.555556  \n6       0.444444     0.555556  \n7       0.361111     0.638889  \n8       0.416667     0.583333  \n9       0.416667     0.583333  \n10      0.472222     0.527778  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>AVG_PREC</th>\n      <th>F1</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>ACC</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained</td>\n      <td>0.521286</td>\n      <td>0.638298</td>\n      <td>0.517241</td>\n      <td>0.833333</td>\n      <td>0.527778</td>\n      <td>0.472222</td>\n      <td>0.527778</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.589758</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.530432</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.599219</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.574563</td>\n      <td>0.680000</td>\n      <td>0.531250</td>\n      <td>0.944444</td>\n      <td>0.555556</td>\n      <td>0.444444</td>\n      <td>0.555556</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.571859</td>\n      <td>0.636364</td>\n      <td>0.538462</td>\n      <td>0.777778</td>\n      <td>0.555556</td>\n      <td>0.444444</td>\n      <td>0.555556</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.613684</td>\n      <td>0.666667</td>\n      <td>0.533333</td>\n      <td>0.888889</td>\n      <td>0.555556</td>\n      <td>0.444444</td>\n      <td>0.555556</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.638155</td>\n      <td>0.711111</td>\n      <td>0.592593</td>\n      <td>0.888889</td>\n      <td>0.638889</td>\n      <td>0.361111</td>\n      <td>0.638889</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.679813</td>\n      <td>0.666667</td>\n      <td>0.555556</td>\n      <td>0.833333</td>\n      <td>0.583333</td>\n      <td>0.416667</td>\n      <td>0.583333</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.664490</td>\n      <td>0.516129</td>\n      <td>0.615385</td>\n      <td>0.444444</td>\n      <td>0.583333</td>\n      <td>0.416667</td>\n      <td>0.583333</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.617817</td>\n      <td>0.413793</td>\n      <td>0.545455</td>\n      <td>0.333333</td>\n      <td>0.527778</td>\n      <td>0.472222</td>\n      <td>0.527778</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Above results are for Intervention data","metadata":{}}]}
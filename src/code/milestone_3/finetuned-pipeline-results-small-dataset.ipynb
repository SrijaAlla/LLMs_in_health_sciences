{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8040624,"sourceType":"datasetVersion","datasetId":4740485},{"sourceId":8240694,"sourceType":"datasetVersion","datasetId":4888399},{"sourceId":8247999,"sourceType":"datasetVersion","datasetId":4893605},{"sourceId":8248829,"sourceType":"datasetVersion","datasetId":4894165},{"sourceId":8256261,"sourceType":"datasetVersion","datasetId":4899707}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\n\nTRAIN_PATH = \"/kaggle/input/nlp-project-dataset/Training data/train.json\"\nDEV_PATH = \"/kaggle/input/nlp-project-dataset/Training data/dev.json\"\nTEST_PATH = \"/kaggle/input/nlp-project-dataset/Training data/test.json\"\n\n###TASK 1\ndef generate_nli_data(file_path):\n    '''\n    Generates data from clinical trials for Task 1: Textual entailment (NLI).\n\n    Parameters:\n        file_path (str): Path to the JSON of the dataset.\n\n    Returns:\n        joint_data: List of training instances in form of \"claim [SEP] evidence_text\" (str)\n        labels: List of labels, either 1 for \"Entailment\" or 0 for \"Contradiction\" (int)\n    '''\n\n    #Read the file.\n    df = pd.read_json(file_path)\n    df = df.transpose()\n\n    #Extract claims and labels. Map labels to binary values (0, 1).\n    claims = df.Statement.tolist()\n    labels = df.Label.tolist()\n    labels = list(map(lambda x : 1 if x == \"Entailment\" else 0, labels))\n#     print(labels)\n\n    #(Prepare to) Extract all evidence sentences from clinical trials\n    evidence_texts = list()\n    if('Secondary_id' in df.columns):\n        primary_cts, secondary_cts = df.Primary_id, df.Secondary_id \n        primary_indices = df.Primary_evidence_index \n        secondary_indices = df.Secondary_evidence_index\n    else:\n        primary_cts = df.Primary_id\n        primary_indices = df.Primary_evidence_index \n\n   \n    sections, types = df.Section_id, df.Type\n\n    #Generate evidence texts for each claim.\n    for claim_id in range(len(claims)):\n        file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n\n        with open(file_name, 'r') as f:\n            data = json.load(f)\n            evidence = \"primary trial: \" \n\n            #Evidence for the primary trial is in form:\n            # \"primary trial: sent_1. sent_2. (...) sent_n.\"           \n            for i in primary_indices[claim_id]:\n                evidence += data[sections[claim_id]][i]\n                evidence += \" \"\n                \n        #If it is a comparative claim, also add evidence sentences from the 2nd trial.\n        if types[claim_id] == \"Comparison\":\n            file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n\n            #Evidence for the secondary trial is in form:\n            # \"| secondary trial: sent_1. sent_2. (...) sent_n.\"\n            with open(file_name, 'r') as f:\n                data = json.load(f)\n                evidence += \" | secondary trial: \"\n                for i in secondary_indices[claim_id]:\n                    evidence += data[sections[claim_id]][i]\n                    evidence += \" \"\n\n        evidence_texts.append(evidence)\n\n    #One training instance is: \"claim [SEP] full_evidence_text\"\n    joint_data = list()\n    for i in range(len(claims)):\n        premise = claims[i]\n        hypothesis = evidence_texts[i]\n        joint = premise + \" [SEP] \" + hypothesis\n        joint_data.append(joint)\n\n    return joint_data, labels\n\n\n###TASK 2\ndef generate_evidence_data(file_path):\n    '''\n    Generates data from clinical trials for Task 2: Evidence Retrieval (/selection).\n\n    Parameters:\n        file_path (str): Path to the JSON of the dataset.\n\n    Returns:\n        joint_data: List of training instances in form of \"claim [SEP] candidate_sentence\" (str)\n        labels: List of labels, 0 if candidate_sentence is not evidence, 1 if it is\n    '''\n\n    #Read the file.\n    df = pd.read_json(file_path)\n    df = df.transpose()\n\n    #Extract claims.\n    claims = df.Statement.tolist()\n\n    #(Prepare to) Extract all evidence sentences from clinical trials\n    primary_cts, secondary_cts = df.Primary_id, df.Secondary_id    \n    primary_indices = df.Primary_evidence_index \n    secondary_indices = df.Secondary_evidence_index\n    sections, types = df.Section_id, df.Type\n\n    primary_evidence_sentences = list()\n    secondary_evidence_sentences = list()\n\n    for idx in range(len(claims)):\n        file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[idx] + \".json\"\n\n        #Create a list of all evidence sentences from the primary trial for this claim.\n        with open(file_name, 'r') as f:\n            data = json.load(f)\n            primary_evidence_sentences.append(data[sections[idx]])\n\n        #If it is a comparative claim, also create a list of secondary-trial evidence sentences.\n        if types[idx] == \"Comparison\":\n            file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[idx] + \".json\"\n\n            with open(file_name, 'r') as f:\n                data = json.load(f)\n                secondary_evidence_sentences.append(data[sections[idx]])\n        else:\n            secondary_evidence_sentences.append(list())\n\n    #Generate training instances in form of \"claim [SEP] candidate_sentence\", \n    joint_data = list()\n\n    #Label is 0 if candidate sentece is not evidence for this claim, 1 if it is   \n    labels = list() \n\n    for claim_id in range(len(claims)):\n        claim = claims[claim_id]\n        primary_sents = primary_evidence_sentences[claim_id]\n\n        for sid in range(len(primary_sents)):\n            candidate_sentence = primary_sents[sid]\n            j = candidate_sentence + \" [SEP] \" + claim\n            joint_data.append(j)\n            labels.append(sid in primary_indices[claim_id])\n\n        if types[claim_id] == \"Comparison\":\n            secondary_sents = secondary_evidence_sentences[claim_id]\n            for sid in range(len(secondary_sents)):\n                candidate_sentence = secondary_sents[sid]\n                j = candidate_sentence + \" [SEP] \" + claim\n                joint_data.append(j)\n                labels.append(sid in secondary_indices[claim_id])\n\n        labels = [1 if l else 0 for l in labels]\n\n    return joint_data, labels","metadata":{"execution":{"iopub.status.busy":"2024-04-28T21:08:39.442403Z","iopub.execute_input":"2024-04-28T21:08:39.442878Z","iopub.status.idle":"2024-04-28T21:08:39.901692Z","shell.execute_reply.started":"2024-04-28T21:08:39.442831Z","shell.execute_reply":"2024-04-28T21:08:39.900738Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport os\nfrom safetensors.torch import load_file\n\nfile_path = \"/kaggle/input/finetuned/model.safetensors\"\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# from prepare_data import generate_nli_data\nbest_f1 = 0.0\n\nTRAIN_PATH = \"/kaggle/input/nlp-project-dataset/Training data/train.json\"\nDEV_PATH = \"/kaggle/input/nlp-project-dataset/Training data/dev.json\"\nTEST_PATH = \"/kaggle/input/nlp-project-dataset/Training data/test.json\"\n\n#Torch dataset used in the models. Consists of encodings of training instances and of labels.\n#One training instance is: BERT_TOKENIZER(\"claim [SEP] evidence_text\").\nclass CtDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\nmodels = [\"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\",\n\"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\",\n\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n\"microsoft/deberta-v2-xlarge-mnli\",\n\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"]\n\ndef compute_metrics(pred):\n#     global best_f1, best_model_path\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    # Standard metrics\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    prec = precision_score(labels, preds, average='binary', zero_division=0)  # Handling division by zero\n    recall = recall_score(labels, preds, average='binary', zero_division=0)   # Handling division by zero\n\n    faithfulness_metric = sum(preds != labels) / len(preds)\n    consistency_metric = sum(preds == labels) / len(preds) \n    \n    metrics = {\n        \"accuracy\": acc, \n        \"precision\": prec, \n        \"recall\": recall, \n        \"f1\": f1,\n        \"faithfulness\": faithfulness_metric,\n        \"consistency\": consistency_metric\n    }\n\n    \n#     if f1 > best_f1:\n#         best_f1 = f1\n#         output_dir = trainer.args.output_dir\n#         best_model_path = os.path.join(output_dir, \"best_model\")\n#         trainer.save_model(\"best-model\")\n# #         trainer.tokenizer.save_pretrained(best_model_path)\n#         print(f\"Best model saved with F1 score: {best_f1:.4f}\")\n    return metrics\n\n\n#Training loop.\ndef train(model_name):\n    #model_name = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n\n    #Load the models. Adjust max instance length to fit your machine.\n    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=1024, use_safetensors = True)\n    model = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/finetuned',\n                                 num_labels=2, ignore_mismatched_sizes=True)\n#     model = AutoModelForSequenceClassification.from_pretrained(model_name,\n#                                  num_labels=2, ignore_mismatched_sizes=True)\n#     model.load_state_dict(torch.load('/kaggle/input/finetuned/model.safetensors'))\n\n\n    #Generate joint claim+[SEP]+evidence data.\n    joint_train, labels_train = generate_nli_data(TRAIN_PATH)\n    joint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n    #Tokenize the data.    \n    encoded_train = tokenizer(joint_train, return_tensors='pt',\n                         truncation_strategy='only_first', add_special_tokens=True, padding=True)\n    encoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                         truncation_strategy='only_first', add_special_tokens=True, padding=True)\n   \n    #Convert data into datasets\n    train_dataset = CtDataset(encoded_train, labels_train)\n    dev_dataset = CtDataset(encoded_dev, labels_dev)\n\n    #Define the batch size to fit your GPU memory.\n    batch_size = 8\n#     print(joint_train)\n\n    logging_steps = len(joint_train) // batch_size\n    output_name = f\"finetuned-model\"\n\n    training_args = TrainingArguments(output_dir=output_name,\n                                 per_device_train_batch_size=batch_size,\n                                 per_device_eval_batch_size=batch_size,\n                                 \n                                 #for faster training time\n                                 dataloader_pin_memory=True, \n                                 dataloader_num_workers=4,\n                                 gradient_accumulation_steps=2,\n                                 fp16=True,\n\n                                 #training hyperparameters\n                                 num_train_epochs=30,\n                                 learning_rate=2e-05,\n                                 weight_decay=0.07,\n                                 warmup_ratio=0.1,\n\n                                 #other parameters\n                                 evaluation_strategy=\"epoch\",\n                                 save_strategy=\"no\",\n                                 disable_tqdm=False,\n                                 logging_steps=logging_steps,\n                                 push_to_hub=False)\n\n    trainer = Trainer(model=model, args=training_args,\n                    compute_metrics=compute_metrics,\n                    train_dataset=train_dataset,\n                    eval_dataset=dev_dataset,\n                    tokenizer=tokenizer )\n    print(f\"Number of epochs: {trainer.args.num_train_epochs}\")\n    print(f\"Batch size: {trainer.args.per_device_train_batch_size}\")\n    print(f\"Learning rate: {trainer.args.learning_rate}\")\n\n    #Start the training process.\n    trainer.train()\n\n    #Save the fine-tuned NLI (textual entailment) model.\n    trainer.save_model(\"model-nli\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T21:08:39.903842Z","iopub.execute_input":"2024-04-28T21:08:39.904319Z","iopub.status.idle":"2024-04-28T21:08:46.782222Z","shell.execute_reply.started":"2024-04-28T21:08:39.904284Z","shell.execute_reply":"2024-04-28T21:08:46.781255Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-28 21:08:44.337011: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 21:08:44.337069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 21:08:44.338525: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"#train('MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\n#train('domenicrosati/debertav3small-NLI4CT')\ntrain('MoritzLaurer/DeBERTa-v3-small-mnli-fever-docnli-ling-2c')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T21:08:46.783402Z","iopub.execute_input":"2024-04-28T21:08:46.783928Z","iopub.status.idle":"2024-04-28T22:46:12.128377Z","shell.execute_reply.started":"2024-04-28T21:08:46.783901Z","shell.execute_reply":"2024-04-28T22:46:12.127024Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Number of epochs: 30\nBatch size: 8\nLearning rate: 2e-05\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrijaalla10\u001b[0m (\u001b[33mlalla-ub\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240428_210859-6mg5rnly</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lalla-ub/huggingface/runs/6mg5rnly' target=\"_blank\">happy-microwave-29</a></strong> to <a href='https://wandb.ai/lalla-ub/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lalla-ub/huggingface' target=\"_blank\">https://wandb.ai/lalla-ub/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lalla-ub/huggingface/runs/6mg5rnly' target=\"_blank\">https://wandb.ai/lalla-ub/huggingface/runs/6mg5rnly</a>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3180' max='3180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3180/3180 1:36:51, Epoch 29/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.720649</td>\n      <td>0.505000</td>\n      <td>0.503185</td>\n      <td>0.790000</td>\n      <td>0.461239</td>\n      <td>0.495000</td>\n      <td>0.505000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.803400</td>\n      <td>0.654574</td>\n      <td>0.595000</td>\n      <td>0.579832</td>\n      <td>0.690000</td>\n      <td>0.591312</td>\n      <td>0.405000</td>\n      <td>0.595000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.680900</td>\n      <td>0.613573</td>\n      <td>0.620000</td>\n      <td>0.657895</td>\n      <td>0.500000</td>\n      <td>0.614448</td>\n      <td>0.380000</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.606600</td>\n      <td>0.668061</td>\n      <td>0.630000</td>\n      <td>0.604839</td>\n      <td>0.750000</td>\n      <td>0.624594</td>\n      <td>0.370000</td>\n      <td>0.630000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.468300</td>\n      <td>0.806576</td>\n      <td>0.670000</td>\n      <td>0.677083</td>\n      <td>0.650000</td>\n      <td>0.669868</td>\n      <td>0.330000</td>\n      <td>0.670000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.372100</td>\n      <td>0.949617</td>\n      <td>0.665000</td>\n      <td>0.660194</td>\n      <td>0.680000</td>\n      <td>0.664925</td>\n      <td>0.335000</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.280900</td>\n      <td>1.158722</td>\n      <td>0.655000</td>\n      <td>0.639640</td>\n      <td>0.710000</td>\n      <td>0.653953</td>\n      <td>0.345000</td>\n      <td>0.655000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.219900</td>\n      <td>1.113714</td>\n      <td>0.690000</td>\n      <td>0.702128</td>\n      <td>0.660000</td>\n      <td>0.689721</td>\n      <td>0.310000</td>\n      <td>0.690000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.189000</td>\n      <td>1.453788</td>\n      <td>0.650000</td>\n      <td>0.650000</td>\n      <td>0.650000</td>\n      <td>0.650000</td>\n      <td>0.350000</td>\n      <td>0.650000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.155300</td>\n      <td>1.667954</td>\n      <td>0.665000</td>\n      <td>0.685393</td>\n      <td>0.610000</td>\n      <td>0.663984</td>\n      <td>0.335000</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.140000</td>\n      <td>2.224527</td>\n      <td>0.640000</td>\n      <td>0.620690</td>\n      <td>0.720000</td>\n      <td>0.637681</td>\n      <td>0.360000</td>\n      <td>0.640000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.119900</td>\n      <td>2.255180</td>\n      <td>0.650000</td>\n      <td>0.636364</td>\n      <td>0.700000</td>\n      <td>0.649123</td>\n      <td>0.350000</td>\n      <td>0.650000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.086300</td>\n      <td>2.372475</td>\n      <td>0.655000</td>\n      <td>0.647619</td>\n      <td>0.680000</td>\n      <td>0.654784</td>\n      <td>0.345000</td>\n      <td>0.655000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.069500</td>\n      <td>2.475043</td>\n      <td>0.655000</td>\n      <td>0.634783</td>\n      <td>0.730000</td>\n      <td>0.653048</td>\n      <td>0.345000</td>\n      <td>0.655000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.068700</td>\n      <td>2.645272</td>\n      <td>0.645000</td>\n      <td>0.633028</td>\n      <td>0.690000</td>\n      <td>0.644280</td>\n      <td>0.355000</td>\n      <td>0.645000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.053200</td>\n      <td>2.638355</td>\n      <td>0.655000</td>\n      <td>0.639640</td>\n      <td>0.710000</td>\n      <td>0.653953</td>\n      <td>0.345000</td>\n      <td>0.655000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('MoritzLaurer/DeBERTa-v3-small-mnli-fever-docnli-ling-2c', model_max_length=1024, use_safetensors = True)\nmodel = AutoModelForSequenceClassification.from_pretrained('/kaggle/working/model-nli',num_labels=2, ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:47:48.790005Z","iopub.execute_input":"2024-04-28T22:47:48.790674Z","iopub.status.idle":"2024-04-28T22:47:51.611496Z","shell.execute_reply.started":"2024-04-28T22:47:48.790633Z","shell.execute_reply":"2024-04-28T22:47:51.610402Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":" #Define the batch size to fit your GPU memory.\nDEV_PATH = '/kaggle/input/extra-sebis/Numerical_Statements.json'\njoint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \nencoded_train = tokenizer(joint_train, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\ntrain_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\n\nlogging_steps = len(joint_train) // batch_size\noutput_name = f\"finetuned-model-eval\"\ntraining_args = TrainingArguments(output_dir=output_name,\n                                 per_device_train_batch_size=batch_size,\n                                 per_device_eval_batch_size=batch_size,\n                                 \n                                 #for faster training time\n                                 dataloader_pin_memory=True, \n                                 dataloader_num_workers=4,\n                                 gradient_accumulation_steps=2,\n                                 fp16=True,\n\n                                 #training hyperparameters\n                                 num_train_epochs=30,\n                                 learning_rate=2e-05,\n                                 weight_decay=0.07,\n                                 warmup_ratio=0.1,\n\n                                 #other parameters\n                                 evaluation_strategy=\"epoch\",\n                                 save_strategy=\"no\",\n                                 disable_tqdm=False,\n                                 logging_steps=logging_steps,\n                                 push_to_hub=False)\n\ntrainer = Trainer(model=model, args=training_args,\n                    compute_metrics=compute_metrics,\n                    train_dataset=train_dataset,\n                    eval_dataset=dev_dataset,\n                    tokenizer=tokenizer )","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:47:55.383786Z","iopub.execute_input":"2024-04-28T22:47:55.384531Z","iopub.status.idle":"2024-04-28T22:48:00.977866Z","shell.execute_reply.started":"2024-04-28T22:47:55.384495Z","shell.execute_reply":"2024-04-28T22:48:00.976422Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"new_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nprint(f\"Evaluation metrics on new dataset: {new_eval_metrics}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:00.980061Z","iopub.execute_input":"2024-04-28T22:48:00.980438Z","iopub.status.idle":"2024-04-28T22:48:02.805513Z","shell.execute_reply.started":"2024-04-28T22:48:00.980407Z","shell.execute_reply":"2024-04-28T22:48:02.804408Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='156' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 01:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation metrics on new dataset: {'eval_loss': 2.8241848945617676, 'eval_accuracy': 0.6451612903225806, 'eval_precision': 0.6046511627906976, 'eval_recall': 0.6190476190476191, 'eval_f1': 0.6454919495744642, 'eval_faithfulness': 0.3548387096774194, 'eval_consistency': 0.6451612903225806, 'eval_runtime': 1.8007, 'eval_samples_per_second': 51.647, 'eval_steps_per_second': 6.664}\n","output_type":"stream"}]},{"cell_type":"code","source":"results = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:05.634664Z","iopub.execute_input":"2024-04-28T22:48:05.635550Z","iopub.status.idle":"2024-04-28T22:48:07.431579Z","shell.execute_reply.started":"2024-04-28T22:48:05.635514Z","shell.execute_reply":"2024-04-28T22:48:07.430492Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   VAL loss       ACC      PREC       REC        F1  Faithfulness  Consistency\n0  2.824185  0.645161  0.604651  0.619048  0.645492      0.354839     0.645161","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.824185</td>\n      <td>0.645161</td>\n      <td>0.604651</td>\n      <td>0.619048</td>\n      <td>0.645492</td>\n      <td>0.354839</td>\n      <td>0.645161</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"DEV_PATH = '/kaggle/input/extra-sebis/Non_Numerical_Statements.json'\n# joint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \n# encoded_train = tokenizer(joint_train, return_tensors='pt',\n#                              truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\n# train_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\nnew_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nresults = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:11.483036Z","iopub.execute_input":"2024-04-28T22:48:11.483710Z","iopub.status.idle":"2024-04-28T22:48:19.998491Z","shell.execute_reply.started":"2024-04-28T22:48:11.483675Z","shell.execute_reply":"2024-04-28T22:48:19.997500Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   VAL loss       ACC      PREC       REC        F1  Faithfulness  Consistency\n0  2.476839  0.663551  0.661765  0.775862  0.657786      0.336449     0.663551","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.476839</td>\n      <td>0.663551</td>\n      <td>0.661765</td>\n      <td>0.775862</td>\n      <td>0.657786</td>\n      <td>0.336449</td>\n      <td>0.663551</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"DEV_PATH = '/kaggle/input/extra-sebis/Comparison.json'\n# joint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \n# encoded_train = tokenizer(joint_train, return_tensors='pt',\n#                              truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\n# train_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\nnew_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nresults = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:32.633926Z","iopub.execute_input":"2024-04-28T22:48:32.634663Z","iopub.status.idle":"2024-04-28T22:48:37.726072Z","shell.execute_reply.started":"2024-04-28T22:48:32.634627Z","shell.execute_reply":"2024-04-28T22:48:37.725169Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   VAL loss  ACC      PREC       REC        F1  Faithfulness  Consistency\n0   3.00234  0.6  0.588235  0.666667  0.598214           0.4          0.6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.00234</td>\n      <td>0.6</td>\n      <td>0.588235</td>\n      <td>0.666667</td>\n      <td>0.598214</td>\n      <td>0.4</td>\n      <td>0.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"DEV_PATH = '/kaggle/input/extra-sebis/Adverse Events.json'\n# joint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \n# encoded_train = tokenizer(joint_train, return_tensors='pt',\n#                              truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\n# train_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\nnew_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nresults = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:42.314999Z","iopub.execute_input":"2024-04-28T22:48:42.315392Z","iopub.status.idle":"2024-04-28T22:48:44.338480Z","shell.execute_reply.started":"2024-04-28T22:48:42.315349Z","shell.execute_reply":"2024-04-28T22:48:44.337418Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   VAL loss       ACC      PREC       REC        F1  Faithfulness  Consistency\n0  2.923334  0.596154  0.580645  0.692308  0.592385      0.403846     0.596154","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.923334</td>\n      <td>0.596154</td>\n      <td>0.580645</td>\n      <td>0.692308</td>\n      <td>0.592385</td>\n      <td>0.403846</td>\n      <td>0.596154</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"DEV_PATH = '/kaggle/input/extra-sebis/Eligibility.json'\n# joint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \n# encoded_train = tokenizer(joint_train, return_tensors='pt',\n#                              truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\n# train_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\nnew_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nresults = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:48.864755Z","iopub.execute_input":"2024-04-28T22:48:48.865635Z","iopub.status.idle":"2024-04-28T22:48:53.591501Z","shell.execute_reply.started":"2024-04-28T22:48:48.865598Z","shell.execute_reply":"2024-04-28T22:48:53.590408Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"   VAL loss   ACC      PREC       REC        F1  Faithfulness  Consistency\n0  1.768191  0.75  0.733333  0.785714  0.749681          0.25         0.75","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.768191</td>\n      <td>0.75</td>\n      <td>0.733333</td>\n      <td>0.785714</td>\n      <td>0.749681</td>\n      <td>0.25</td>\n      <td>0.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"DEV_PATH = '/kaggle/input/extra-sebis/Intervention.json'\n# joint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \n# encoded_train = tokenizer(joint_train, return_tensors='pt',\n#                              truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\n# train_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\nnew_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nresults = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:53.593425Z","iopub.execute_input":"2024-04-28T22:48:53.593713Z","iopub.status.idle":"2024-04-28T22:48:55.090243Z","shell.execute_reply.started":"2024-04-28T22:48:53.593686Z","shell.execute_reply":"2024-04-28T22:48:55.089176Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   VAL loss       ACC      PREC       REC        F1  Faithfulness  Consistency\n0  2.985244  0.583333  0.578947  0.611111  0.583012      0.416667     0.583333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.985244</td>\n      <td>0.583333</td>\n      <td>0.578947</td>\n      <td>0.611111</td>\n      <td>0.583012</td>\n      <td>0.416667</td>\n      <td>0.583333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"DEV_PATH = '/kaggle/input/extra-sebis/Results.json'\n# joint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \n# encoded_train = tokenizer(joint_train, return_tensors='pt',\n#                              truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\n# train_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\nnew_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nresults = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:55.093825Z","iopub.execute_input":"2024-04-28T22:48:55.094152Z","iopub.status.idle":"2024-04-28T22:48:57.624821Z","shell.execute_reply.started":"2024-04-28T22:48:55.094121Z","shell.execute_reply":"2024-04-28T22:48:57.620233Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/tmp/ipykernel_9485/796109816.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + secondary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in secondary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"   VAL loss       ACC      PREC       REC        F1  Faithfulness  Consistency\n0  3.020997  0.660714  0.645161  0.714286  0.659738      0.339286     0.660714","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.020997</td>\n      <td>0.660714</td>\n      <td>0.645161</td>\n      <td>0.714286</td>\n      <td>0.659738</td>\n      <td>0.339286</td>\n      <td>0.660714</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"DEV_PATH = '/kaggle/input/extra-sebis/single.json'\n# joint_train, labels_train = generate_nli_data(TRAIN_PATH)\njoint_dev, labels_dev= generate_nli_data(DEV_PATH)\n\n        #Tokenize the data.    \n# encoded_train = tokenizer(joint_train, return_tensors='pt',\n#                              truncation_strategy='only_first', add_special_tokens=True, padding=True)\nencoded_dev = tokenizer(joint_dev, return_tensors='pt',\n                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n\n        #Convert data into datasets\n# train_dataset = CtDataset(encoded_train, labels_train)\ndev_dataset = CtDataset(encoded_dev, labels_dev)\nbatch_size = 8\nnew_eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\nresults = []\n# for epoch in range(trainer.args.num_train_epochs):\nfor epoch in range(1):\n    eval_metrics = trainer.evaluate(eval_dataset=dev_dataset)\n    results.append([\n        eval_metrics['eval_loss'],\n        eval_metrics['eval_accuracy'],\n        eval_metrics['eval_precision'],\n        eval_metrics['eval_recall'],\n        eval_metrics['eval_f1'],\n        eval_metrics['eval_faithfulness'],\n        eval_metrics['eval_consistency'],        \n    ])\ncolumns = ['VAL loss', 'ACC', 'PREC', 'REC', 'F1', 'Faithfulness', 'Consistency']\npd.DataFrame(results, columns=columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T22:48:57.627583Z","iopub.execute_input":"2024-04-28T22:48:57.627956Z","iopub.status.idle":"2024-04-28T22:49:02.909632Z","shell.execute_reply.started":"2024-04-28T22:48:57.627925Z","shell.execute_reply":"2024-04-28T22:49:02.908604Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_9485/796109816.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  file_name = \"/kaggle/input/nlp-project-dataset/Training data/CT json/\" + primary_cts[claim_id] + \".json\"\n/tmp/ipykernel_9485/796109816.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  for i in primary_indices[claim_id]:\n/tmp/ipykernel_9485/796109816.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  evidence += data[sections[claim_id]][i]\n/tmp/ipykernel_9485/796109816.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if types[claim_id] == \"Comparison\":\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n  warnings.warn(\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/tmp/ipykernel_9485/2714903670.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   VAL loss       ACC      PREC       REC        F1  Faithfulness  Consistency\n0  2.482361  0.678571  0.662338  0.728571  0.677766      0.321429     0.678571","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VAL loss</th>\n      <th>ACC</th>\n      <th>PREC</th>\n      <th>REC</th>\n      <th>F1</th>\n      <th>Faithfulness</th>\n      <th>Consistency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.482361</td>\n      <td>0.678571</td>\n      <td>0.662338</td>\n      <td>0.728571</td>\n      <td>0.677766</td>\n      <td>0.321429</td>\n      <td>0.678571</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}